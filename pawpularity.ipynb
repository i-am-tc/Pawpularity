{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "pawpularity.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAC8KWATdCK8"
      },
      "source": [
        "Credits\n",
        "\n",
        "Tez Training by Abhishek - https://www.kaggle.com/abhishek/tez-pawpular-training/notebook\n",
        "\n",
        "RAPIDS SVR by Chris - https://www.kaggle.com/cdeotte/rapids-svr-boost-17-8?scriptVersionId=76428219\n",
        "\n",
        "Fold creation - https://www.kaggle.com/abhishek/same-old-creating-folds \n",
        "\n",
        "Install RAPIDS in Colab - https://is.gd/KOS1nC \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XezE-cWddCK_"
      },
      "source": [
        "# Arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:04.115960Z",
          "iopub.execute_input": "2021-11-23T15:19:04.116277Z",
          "iopub.status.idle": "2021-11-23T15:19:04.123119Z",
          "shell.execute_reply.started": "2021-11-23T15:19:04.116237Z",
          "shell.execute_reply": "2021-11-23T15:19:04.121953Z"
        },
        "trusted": true,
        "id": "ru3xjMRXdCLA"
      },
      "source": [
        "# Simple class to hold a bunch of globals \n",
        "class args:\n",
        "    \n",
        "    # Where you developing? So we can use P100 from both. =)\n",
        "    platform = 'colab' # colab or kaggle\n",
        "    \n",
        "    # Where to download or upload stuff\n",
        "    # Sink location should preserve outputs even if runtime disconns.\n",
        "    source = 'drive' # 's3' or 'drive' or 'kaggle'\n",
        "    sink = 's3' # 's3' or kaggle\n",
        "    \n",
        "    # Depending on where, these paths will come in handy\n",
        "    if platform == 'kaggle':\n",
        "        csv_path = \"../input/petfinder-pawpularity-score/\"\n",
        "        model_path = \"../input/pawpularity-model-files/\"\n",
        "        image_path = \"../input/petfinder-pawpularity-score/train/\"\n",
        "\n",
        "    if platform == 'colab':\n",
        "        csv_path = '/content/'\n",
        "        model_path = '/content/'\n",
        "        image_path = '/content/img/'\n",
        "    \n",
        "    # Are we creating a new fold CSV file?\n",
        "    # Rmb we take original data CSV and add a col for fold\n",
        "    isSaveFold = False\n",
        "    \n",
        "    # Model training\n",
        "    batch_size = 32\n",
        "    image_size = 256\n",
        "    epochs = 20\n",
        "    fold = 0\n",
        "    \n",
        "    # Activity control switches\n",
        "    isNNtraining = True\n",
        "    isSVRfitting = False\n",
        "    isGPU = True\n",
        "    isTPU = False\n",
        "    isOOF = True\n",
        "    isTEST = False"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHHSzIvXdMx0"
      },
      "source": [
        "# Installs & Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:04.125003Z",
          "iopub.execute_input": "2021-11-23T15:19:04.125381Z",
          "iopub.status.idle": "2021-11-23T15:19:04.143710Z",
          "shell.execute_reply.started": "2021-11-23T15:19:04.125338Z",
          "shell.execute_reply": "2021-11-23T15:19:04.142983Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kqgn2kZsdCLB",
        "outputId": "f69edbea-0182-4519-974a-a6a7c8c96876"
      },
      "source": [
        "# Credit https://www.kaggle.com/abhishek/tez-pawpular-swin-ference\n",
        "# based on the post here: https://www.kaggle.com/c/petfinder-pawpularity-score/discussion/275094\n",
        "\n",
        "# Silencing this cell's output.\n",
        "# %%capture\n",
        "\n",
        "# Standard ones\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import pickle \n",
        "import subprocess\n",
        "\n",
        "# Installs & path includes\n",
        "if args.platform == 'kaggle':\n",
        "    sys.path.append(\"../input/tez-lib/\")\n",
        "    sys.path.append(\"../input/timmmaster/\")\n",
        "    \n",
        "if args.platform == 'colab':\n",
        "    subprocess.run('pip install tez', shell=True)\n",
        "    subprocess.run('pip install timm', shell=True)\n",
        "    subprocess.run('pip install boto3', shell=True)\n",
        "    subprocess.run('apt install unzip', shell=True)\n",
        "\n",
        "# Analysis\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn import datasets\n",
        "from sklearn import model_selection\n",
        "import albumentations # https://is.gd/ngksFx ; for image augmentation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm # https://is.gd/suAm9l ; Pytorch vision library\n",
        "import tez # https://git.io/J1KCq ; convenience library for using Pytorch\n",
        "from tez.callbacks import EarlyStopping\n",
        "\n",
        "# For GPU enabled support vector machines\n",
        "if args.platform == 'kaggle':\n",
        "    # We can direclty import in kaggle as it is included by default.\n",
        "    import cudf, cuml # Doc - https://is.gd/oshcbU ; \n",
        "    from cuml.svm import SVR\n",
        "    print('RAPIDS version',cuml.__version__,'\\n')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:04.145839Z",
          "iopub.execute_input": "2021-11-23T15:19:04.146127Z",
          "iopub.status.idle": "2021-11-23T15:19:04.155646Z",
          "shell.execute_reply.started": "2021-11-23T15:19:04.146092Z",
          "shell.execute_reply": "2021-11-23T15:19:04.154900Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee-hxp7KdCLC",
        "outputId": "975ebb1f-a006-4a7f-bed1-83d7c7183433"
      },
      "source": [
        "if args.source == 'drive':\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True) \n",
        "\n",
        "if args.source == 's3' or args.sink == 's3':\n",
        "\n",
        "    # Setting up for AWS S3\n",
        "\n",
        "    import boto3 # Doc - https://is.gd/K9zpHw \n",
        "    from getpass import getpass\n",
        "\n",
        "    BUCKET_NAME = 'pawpularity-data'\n",
        "\n",
        "    # Using getpass from here https://is.gd/yN9yap for security. \n",
        "\n",
        "    print('Input AWS access key ID:')\n",
        "    aws_access_key_id = getpass()\n",
        "    print('Input AWS secret access key:')\n",
        "    aws_secret_access_key = getpass()\n",
        "\n",
        "    s3r = boto3.resource('s3', \n",
        "                        aws_access_key_id = aws_access_key_id, \n",
        "                        aws_secret_access_key= aws_secret_access_key) \n",
        "\n",
        "    s3c = boto3.client('s3', \n",
        "                        aws_access_key_id = aws_access_key_id, \n",
        "                        aws_secret_access_key= aws_secret_access_key)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Input AWS access key ID:\n",
            "··········\n",
            "Input AWS secret access key:\n",
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:04.157674Z",
          "iopub.execute_input": "2021-11-23T15:19:04.157995Z",
          "iopub.status.idle": "2021-11-23T15:19:04.169704Z",
          "shell.execute_reply.started": "2021-11-23T15:19:04.157959Z",
          "shell.execute_reply": "2021-11-23T15:19:04.168981Z"
        },
        "trusted": true,
        "id": "QwooOYZOdCLC"
      },
      "source": [
        "# Credit: https://is.gd/zwVtpa \n",
        "# Helper function, takes in an original data csv, returns pd Series ready for cross-val\n",
        "\n",
        "def create_folds(data, num_splits):\n",
        "\n",
        "    # Added col \"kfold\", assigned val -1 for starters\n",
        "    data[\"kfold\"] = -1 \n",
        "\n",
        "    # https://is.gd/wgZBrH & https://www.statisticshowto.com/?p=7678\n",
        "    # Rule-thumb for setting bin sizes, most likely Sturge rule used here\n",
        "    # bins are akin to giving labels to equal width range of pscore. \n",
        "    num_bins = int(np.floor(1 + np.log2(len(data)))) \n",
        "    \n",
        "    # https://is.gd/U6dVgC, sort by pscore, segment into num_bins, output into col \"bins\": bin 1, bin 2 ...\n",
        "    data.loc[:, \"bins\"] = pd.cut(data[\"Pawpularity\"], bins=num_bins, labels=False) \n",
        "\n",
        "    # Doc: https://is.gd/6MJHst, What: https://is.gd/pG4oqH , Why: https://is.gd/bVYvsS\n",
        "    # Instantiate a stratifed k-fold cross validator object, this gives us 90%/10% split betw train/valid\n",
        "    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
        "    \n",
        "    # .split generates indices for split betw train/valid, accord to bin labels\n",
        "    # enumerate(kf.split(X=data, y=data.bins.values)) is of shape (10,2) , col 1 is fold label, col 2 is a tuple of entry-index of (train, valid)\n",
        "    # What's enumerate: https://www.programiz.com/node/600 : adds col 1's fold label\n",
        "    # For each fold label, use .loc to assign list of valid-index corresponding fold label\n",
        "    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n",
        "        data.loc[v_, 'kfold'] = f\n",
        "    \n",
        "    # Delte bin label col as we no longer need it.\n",
        "    data = data.drop(\"bins\", axis=1)\n",
        "\n",
        "    # Return an edited dataframe\n",
        "    return data\n",
        "\n",
        "if args.isSaveFold:\n",
        "    # Save new folds to csv\n",
        "    try:\n",
        "        df_10.to_csv(\"train_10folds.csv\", index=False)\n",
        "    except:\n",
        "        print(\"Could not save to CSV.\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:04.171219Z",
          "iopub.execute_input": "2021-11-23T15:19:04.171547Z",
          "iopub.status.idle": "2021-11-23T15:19:04.215929Z",
          "shell.execute_reply.started": "2021-11-23T15:19:04.171511Z",
          "shell.execute_reply": "2021-11-23T15:19:04.215238Z"
        },
        "trusted": true,
        "id": "QYV-P5iJdCLD"
      },
      "source": [
        "# Get CSVs and images.\n",
        "\n",
        "if args.source == 's3':\n",
        "    \n",
        "    # Download CSVs from S3\n",
        "    s3r.Object(BUCKET_NAME, 'train.csv').download_file('train.csv')\n",
        "    s3r.Object(BUCKET_NAME, 'test.csv').download_file('test.csv')\n",
        "    s3r.Object(BUCKET_NAME, 'train_10folds.csv').download_file('train_10folds.csv')\n",
        "\n",
        "    # Read original csv\n",
        "    df = pd.read_csv(args.path+\"train_10folds.csv\")\n",
        "    \n",
        "    # Read test csv\n",
        "    df_test = pd.read_csv(args.path+\"test.csv\")\n",
        "\n",
        "if args.source == 'drive':\n",
        "    \n",
        "    # Unzip and copy over\n",
        "    subprocess.run('unzip -j /content/drive/MyDrive/Pawpularity_TC_Drive/petfinder-pawpularity-score.zip -d img/', shell=True)\n",
        "    subprocess.run('mv /content/img/train.csv /content', shell=True)\n",
        "    subprocess.run('mv /content/img/test.csv /content', shell=True)\n",
        "    subprocess.run('rm -rf /content/img/test /content/img/sample_submission.csv', shell=True)\n",
        "    \n",
        "    # Read original csv\n",
        "    df = create_folds( pd.read_csv(\"train.csv\") , num_splits=10)\n",
        "\n",
        "    # Read test csv\n",
        "    df_test = pd.read_csv(\"test.csv\")\n",
        "    \n",
        "if args.source == 'kaggle':\n",
        "    # Read original csv\n",
        "    df = create_folds( pd.read_csv(args.csv_path+\"train.csv\") , num_splits=10)\n",
        "\n",
        "    # Read test csv\n",
        "    df_test = pd.read_csv(args.csv_path+\"test.csv\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:04.397527Z",
          "iopub.execute_input": "2021-11-23T15:19:04.399141Z",
          "iopub.status.idle": "2021-11-23T15:19:04.410956Z",
          "shell.execute_reply.started": "2021-11-23T15:19:04.399109Z",
          "shell.execute_reply": "2021-11-23T15:19:04.410153Z"
        },
        "trusted": true,
        "id": "_n8GKuKddCLE"
      },
      "source": [
        "# Credit and edits made: https://is.gd/jEPCoQ\n",
        "# mknod : https://is.gd/OVXJsr \n",
        "# https://masnun.com/?p=3009: Tutorial on Python's concurrent & futures\n",
        "# https://is.gd/S1x8tA : When to ThresdPool and ProcessPool\n",
        "\n",
        "# Get images from S3. Peeled out because this takes longest and costs tiny $$\n",
        "if args.source == 's3':\n",
        "    from concurrent import futures\n",
        "\n",
        "    prefix = 'img'\n",
        "    bucket_name = 'pawpularity-data'\n",
        "    max_workers = 20000\n",
        "\n",
        "    # Saving strings of keys of images ; Since we want to be S3 compatible, we'll need prefix + \"/\" + image name + .jpg\n",
        "    img_keys = [prefix+\"/\"+str(x)+\".jpg\" for x in df[\"Id\"].values]\n",
        "    abs_path = os.path.abspath('')\n",
        "\n",
        "    try:\n",
        "        os.makedirs('./'+ prefix)\n",
        "    except:\n",
        "        print(\"Directory already created. Moving on ...\")\n",
        "\n",
        "    def fetch(key):\n",
        "        file = f'{abs_path}/{key}'\n",
        "        os.mknod(file, mode=384)  \n",
        "        with open(file, 'wb') as data:\n",
        "            s3c.download_fileobj(bucket_name, key, data)\n",
        "        return file\n",
        "\n",
        "    def fetch_all(keys):\n",
        "\n",
        "        with futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "\n",
        "            print(\"Hang on ... submitting file downloads\")\n",
        "\n",
        "            future_to_key = {executor.submit(fetch, key): key for key in keys}\n",
        "\n",
        "            print(\"All URLs submitted.\")\n",
        "\n",
        "            for future in futures.as_completed(future_to_key):\n",
        "\n",
        "                key = future_to_key[future]\n",
        "                exception = future.exception()\n",
        "\n",
        "                if not exception:\n",
        "                    yield key, future.result()\n",
        "                else:\n",
        "                    yield key, exception\n",
        "\n",
        "    i=0\n",
        "    for key, result in fetch_all(img_keys):\n",
        "        i+=1\n",
        "\n",
        "    print('Number of images downloaded: ', i)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ABhJSI4e-W3"
      },
      "source": [
        "# Helper functions + Data & Model Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:04.373265Z",
          "iopub.execute_input": "2021-11-23T15:19:04.373491Z",
          "iopub.status.idle": "2021-11-23T15:19:04.395839Z",
          "shell.execute_reply.started": "2021-11-23T15:19:04.373465Z",
          "shell.execute_reply": "2021-11-23T15:19:04.395126Z"
        },
        "trusted": true,
        "id": "krLJwTwWdCLD"
      },
      "source": [
        "# Saving strings of name of metadata given in train.csv\n",
        "# We use this when we instantiate a Pawpular dataset, to tell it what metadata to expect.\n",
        "dense_features = [\n",
        "    'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n",
        "    'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'\n",
        "]\n",
        "\n",
        "# Helper: sigmoid, math for 1 of many types of activation function.\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + math.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def gauss(x):\n",
        "    return np.exp(-(x*x))\n",
        "\n",
        "# Class to represent dataset's image , metadata & labels , sushi rolled into 1 object. \n",
        "# Returns dict of torch tensors form of image, metadata & labels\n",
        "class PawpularDataset:\n",
        "\n",
        "    # Method called when object is instantiated, here, we set what params to take in\n",
        "    # https://www.geeksforgeeks.org/?p=360686 \n",
        "    def __init__(self, image_paths, dense_features, targets, augmentations):\n",
        "        self.image_paths = image_paths\n",
        "        self.dense_features = dense_features\n",
        "        self.targets = targets\n",
        "        self.augmentations = augmentations\n",
        "    \n",
        "    # Gives length of an attribute here when using len() on an instance\n",
        "    # https://www.analyticsvidhya.com/?p=83204#h2_5 \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    \n",
        "    # https://www.geeksforgeeks.org/?p=385574 \n",
        "    # https://www.codespeedy.com/?p=28884 : setitem vs getitem \n",
        "    # __getitem__ called when we -> InstanceOfDataSet[0] \n",
        "    # __setitem__ called when we -> InstanceOfDataSet[0] = *something*\n",
        "    def __getitem__(self, item):\n",
        "\n",
        "        # Use cv2 to read image with path\n",
        "        image = cv2.imread(self.image_paths[item])\n",
        "\n",
        "        # Why: https://is.gd/eSX1xj\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
        "        \n",
        "        # If augm obj passed in, we perform augm and return augmented images\n",
        "        if self.augmentations is not None:\n",
        "\n",
        "            # https://is.gd/01LBW6, do the augm\n",
        "            augmented = self.augmentations(image=image)\n",
        "\n",
        "            # Albumentation returns a dictionary after augm, only a single key. \n",
        "            image = augmented[\"image\"]\n",
        "\n",
        "        # https://is.gd/nPDsJf, why are we transposing here? \n",
        "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
        "\n",
        "        # Get image's dense_features\n",
        "        features = self.dense_features[item, :]\n",
        "\n",
        "        # Get image's targets\n",
        "        targets = self.targets[item]\n",
        "        \n",
        "        return {\n",
        "            \"image\": torch.tensor(image, dtype=torch.float),\n",
        "            \"features\": torch.tensor(features, dtype=torch.float),\n",
        "            \"targets\": torch.tensor(targets, dtype=torch.float),\n",
        "        }\n",
        "    \n",
        "class PawpularModel(tez.Model):\n",
        "    def __init__(self, get_pretrained, model_string):\n",
        "        \n",
        "        # Inherit tez.Model, https://git.io/J1VAJ , tez.Model in turn inherits nn.Module\n",
        "        super().__init__()\n",
        "        \n",
        "        self.get_pretrained = get_pretrained\n",
        "        self.model_string = model_string\n",
        "        \n",
        "        # Use of pretrained tf_efficientnet_b0_ns as base\n",
        "        # https://is.gd/pOZdAH : List of models supported by timm \n",
        "        # https://git.io/J1VNa : Useful links to Efficient net family\n",
        "        # https://is.gd/zCLbMt: What create_model does\n",
        "        # https://git.io/J1weq : where create_model is in timm's code\n",
        "        # create_model > load_checkpoint > load_pretrained \n",
        "        # A sort of object returned, seems to download from somewhere, rather than actually creating it. Bunch of checks along the way        \n",
        "        self.model = timm.create_model(self.model_string, pretrained=self.get_pretrained, in_chans=3)\n",
        "        \n",
        "        # self.model.classifier is a module of the model above, a pre-trained model created with timm, consisting of deep sequence of diff modules\n",
        "        # https://git.io/J1wmw : EfficientNet class in Pytorch\n",
        "        # https://git.io/J1wmH : nn.Module class in torch\n",
        "        # Output features value from default of 1280 to 128\n",
        "        self.model.classifier = nn.Linear(self.model.classifier.in_features, 128) # Use with EffNet\n",
        "        #self.model.head = nn.Linear(self.model.head.in_features, 128) # Use with SWIN\n",
        "\n",
        "        # https://is.gd/joBsSF : form of regularization technique, zero-ing elements in a tensor with a Bernoulli distri with param p=0.1\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "\n",
        "        # https://is.gd/TnIKzT : Defining MLP layers\n",
        "        # 128+12 input as from model above\n",
        "        # 1 output since we're doing regreession of pscore here. \n",
        "        self.dense1 = nn.Linear(140, 64)\n",
        "        self.dense2 = nn.Linear(64, 1)\n",
        "        \n",
        "        self.step_scheduler_after = \"epoch\"\n",
        "\n",
        "    # RMSE - https://is.gd/1700Cd \n",
        "    # RMSE reporting when target is passed in. \n",
        "    def monitor_metrics(self, outputs, targets):\n",
        "        if args.isNNtraining:\n",
        "            outputs = outputs.cpu().detach().numpy()\n",
        "            targets = targets.cpu().detach().numpy()\n",
        "            rmse = metrics.mean_squared_error(targets, outputs, squared=False)\n",
        "            return {\"rmse\": rmse}\n",
        "        else:\n",
        "            return \n",
        "\n",
        "    # https://is.gd/gMaUfO : control Learning rate decay, similar to that we learnt in SGD-Module 2-MLPy in MM, more spohisticated. \n",
        "    def fetch_scheduler(self):\n",
        "        sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            self.optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1\n",
        "        )\n",
        "        return sch\n",
        "\n",
        "    # https://is.gd/N2C9eB : ADAM optimizer\n",
        "    def fetch_optimizer(self):\n",
        "        opt = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
        "        return opt\n",
        "\n",
        "    def forward(self, image, features, targets=None):\n",
        "        \n",
        "        # send an image into a model\n",
        "        x1 = self.model(image)\n",
        "        \n",
        "        # Apply dropout @ 10%\n",
        "        x = self.dropout(x1)\n",
        "        \n",
        "        # Concatenate dropout output & metadata features given, \n",
        "        x = torch.cat([x, features], dim=1)\n",
        "        \n",
        "        # A MLP's single layer: 128+1 in , 64 out\n",
        "        x = self.dense1(x)\n",
        "        \n",
        "        # A MLP's single layer: 64 in, 1 out\n",
        "        x = self.dense2(x)\n",
        "\n",
        "        if not args.isNNtraining:\n",
        "            # https://is.gd/YIDi42\n",
        "            # If we're not training, return MLP output, image, metadata features\n",
        "            x = torch.cat([x, x1, features], dim=1)\n",
        "\n",
        "        if targets is not None:\n",
        "            loss = nn.MSELoss()(x, targets.view(-1, 1))\n",
        "            metrics = self.monitor_metrics(x, targets)\n",
        "            return x, loss, metrics        \n",
        "        \n",
        "        return x, 0, {}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zxy6Fr4VfhJi"
      },
      "source": [
        "# Image Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:04.412148Z",
          "iopub.execute_input": "2021-11-23T15:19:04.412741Z",
          "iopub.status.idle": "2021-11-23T15:19:04.424240Z",
          "shell.execute_reply.started": "2021-11-23T15:19:04.412700Z",
          "shell.execute_reply": "2021-11-23T15:19:04.423553Z"
        },
        "trusted": true,
        "id": "YzHgko9kdCLF"
      },
      "source": [
        "# Albumentation is a lib for image augmentation operations. \n",
        "# .Compose is the way we define an augmentation pipe line with Albumentation, See https://is.gd/tn84mO , https://is.gd/07Sh95 \n",
        "# list of transforms supported : https://is.gd/weLPx8\n",
        "\n",
        "train_aug = albumentations.Compose(\n",
        "    [\n",
        "        albumentations.Resize(args.image_size, args.image_size, p=1), # https://is.gd/J26M4R\n",
        "        albumentations.HueSaturationValue( # https://is.gd/NlVygf\n",
        "            hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5\n",
        "        ),\n",
        "        albumentations.RandomBrightnessContrast( # https://is.gd/rSiA5P\n",
        "            brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5\n",
        "        ),\n",
        "        albumentations.Normalize( # https://is.gd/GQ4pFo, values used here are defaults\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225],\n",
        "            max_pixel_value=255.0,\n",
        "            p=1.0,\n",
        "        ),\n",
        "    ],\n",
        "    p=1.0,\n",
        ")\n",
        "\n",
        "# Validation set performs only resizing & normalize. Presumably to match images from train set\n",
        "valid_aug = albumentations.Compose(\n",
        "    [\n",
        "        albumentations.Resize(args.image_size, args.image_size, p=1),\n",
        "        albumentations.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225],\n",
        "            max_pixel_value=255.0,\n",
        "            p=1.0,\n",
        "        ),\n",
        "    ],\n",
        "    p=1.0,\n",
        ")\n",
        "\n",
        "test_aug = albumentations.Compose(\n",
        "    [\n",
        "        albumentations.Resize(args.image_size, args.image_size, p=1),\n",
        "        albumentations.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225],\n",
        "            max_pixel_value=255.0,\n",
        "            p=1.0,\n",
        "        ),\n",
        "    ],\n",
        "    p=1.0,\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jGC8rJCiRJb"
      },
      "source": [
        "timm.list_models()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvEIomW0feC8"
      },
      "source": [
        "# Neural Net Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:04.427069Z",
          "iopub.execute_input": "2021-11-23T15:19:04.427546Z",
          "iopub.status.idle": "2021-11-23T15:19:05.380699Z",
          "shell.execute_reply.started": "2021-11-23T15:19:04.427510Z",
          "shell.execute_reply": "2021-11-23T15:19:05.378695Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFN1VBkbdCLF",
        "outputId": "7eaae245-539e-4aed-c46f-59e124a260d0"
      },
      "source": [
        "# Clear out memory that GPU is hogging\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "!export PYTORCH_CUDA_ALLOC_CONF=0\n",
        "\n",
        "# Different fold sets for convenience\n",
        "folds1 = range(10)\n",
        "folds2 = [0,1,2,3,4,5,6,7,8,9]\n",
        "folds3 = [0]\n",
        "\n",
        "if args.isNNtraining:\n",
        "\n",
        "    # Instantiate model, in prep for train.\n",
        "    model = PawpularModel(True, \"tf_efficientnetv2_b0\") # or swin_large_patch4_window12_384\n",
        "    \n",
        "    for i in folds3:\n",
        "        args.fold = i\n",
        "        print(\"\\nTraining fold number:\", args.fold)\n",
        "\n",
        "        # Setting up dataframe for this particular fold\n",
        "        df_train = df[df.kfold != args.fold].reset_index(drop=True)\n",
        "        df_valid = df[df.kfold == args.fold].reset_index(drop=True)\n",
        "\n",
        "        # Adding in full path so model will take this in to know where to expect to find images for training\n",
        "        # remove '/content/' if running on Kaggle \n",
        "        train_img_paths = [args.image_path+f\"{x}.jpg\" for x in df_train[\"Id\"].values]\n",
        "        valid_img_paths = [args.image_path+f\"{x}.jpg\" for x in df_valid[\"Id\"].values]\n",
        "\n",
        "        # Instantiating PawpularDataset objects: 1 for training, 1 for validation\n",
        "        train_dataset = PawpularDataset(\n",
        "            image_paths=train_img_paths,\n",
        "            dense_features=df_train[dense_features].values,\n",
        "            targets=df_train.Pawpularity.values/100.0,\n",
        "            augmentations=train_aug,\n",
        "        )\n",
        "\n",
        "        valid_dataset = PawpularDataset(\n",
        "            image_paths=valid_img_paths,\n",
        "            dense_features=df_valid[dense_features].values,\n",
        "            targets=df_valid.Pawpularity.values/100.0,\n",
        "            augmentations=valid_aug,\n",
        "        )\n",
        "\n",
        "        nn_model_name = f\"model_f{args.fold}.bin\"\n",
        "\n",
        "        # Defining an early stop callback function\n",
        "        es = EarlyStopping(\n",
        "            monitor=\"valid_rmse\",\n",
        "            model_path= nn_model_name,\n",
        "            patience=2,\n",
        "            mode=\"min\",\n",
        "            save_weights_only=True,\n",
        "        )\n",
        "\n",
        "        # Hit the gym and train!!\n",
        "        model.fit(\n",
        "            train_dataset,\n",
        "            valid_dataset=valid_dataset,\n",
        "            train_bs=args.batch_size,\n",
        "            valid_bs=2*args.batch_size,\n",
        "            device=\"cuda\",\n",
        "            epochs=args.epochs,\n",
        "            callbacks=[es],\n",
        "            fp16=True,\n",
        "        )\n",
        "        \n",
        "        if args.sink == 's3':\n",
        "            nn_prefix_name = 'train_effb0_v2_MLP_23Nov/'\n",
        "            # Send model bin file to S3\n",
        "            s3r.meta.client.upload_file(nn_model_name, BUCKET_NAME, nn_prefix_name+nn_model_name)\n",
        "            print(\"\\nUploaded trained model to S3. Fold : \", args.fold)\n",
        "        \n",
        "        if args.sink == 'kaggle':\n",
        "            print('Model output sink set to kaggle. Are you sure? *future improvement')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-effv2-weights/tf_efficientnetv2_b0-c7cc451f.pth\" to /root/.cache/torch/hub/checkpoints/tf_efficientnetv2_b0-c7cc451f.pth\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training fold number: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 279/279 [02:23<00:00,  1.95it/s, loss=0.0411, rmse=0.199, stage=train]\n",
            "  0%|          | 0/16 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 16/16 [00:13<00:00,  1.16it/s, loss=0.0358, rmse=0.188, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score improved (inf --> 0.18779887910932302). Saving model!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/279 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 279/279 [02:23<00:00,  1.94it/s, loss=0.0261, rmse=0.16, stage=train]\n",
            "  0%|          | 0/16 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 16/16 [00:13<00:00,  1.16it/s, loss=0.0376, rmse=0.192, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EarlyStopping counter: 1 out of 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/279 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 279/279 [02:23<00:00,  1.94it/s, loss=0.0146, rmse=0.119, stage=train]\n",
            "  0%|          | 0/16 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 16/16 [00:13<00:00,  1.16it/s, loss=0.0407, rmse=0.2, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EarlyStopping counter: 2 out of 2\n",
            "\n",
            "Uploaded trained model to S3. Fold :  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHhK4WyzdCLG"
      },
      "source": [
        "# Neural Net + Support Vector Regression "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:05.383580Z",
          "iopub.execute_input": "2021-11-23T15:19:05.384239Z",
          "iopub.status.idle": "2021-11-23T15:19:20.855211Z",
          "shell.execute_reply.started": "2021-11-23T15:19:05.384158Z",
          "shell.execute_reply": "2021-11-23T15:19:20.854112Z"
        },
        "trusted": true,
        "id": "XPsd_5_adCLG"
      },
      "source": [
        "# Incorporating RAPIDS's Support Vector Regression\n",
        "\n",
        "# Different fold sets for convenience\n",
        "folds1 = range(10)\n",
        "folds2 = [0,1,2,3,4,5,6,7,8,9]\n",
        "folds3 = [0]\n",
        "\n",
        "# Where to get SVR model files from?\n",
        "\n",
        "if not args.isSVRfitting and not args.isTEST:\n",
        "    if args.source == 's3':\n",
        "        s3r.Object(BUCKET_NAME, \"trained_SVR_23Nov/SVR_fold_0.pkl\").download_file(\"SVR_fold_0.pkl\")\n",
        "    LOAD_SVR_FROM_PATH = './'\n",
        "    \n",
        "if args.isTEST:\n",
        "    LOAD_SVR_FROM_PATH = args.model_path\n",
        "\n",
        "# Adding in full path so model will take this in to know where to expect to find images for training\n",
        "test_img_paths = [args.csv_path+\"test/\"+f\"{x}.jpg\" for x in df_test[\"Id\"].values]\n",
        "\n",
        "# for fold in range(10):\n",
        "for fold in [0]:\n",
        "    \n",
        "    args.fold = fold\n",
        "\n",
        "    # We re-initialize the model at every fold\n",
        "    model = PawpularModel(False, \"tf_efficientnet_b0_ns\") # swin_large_patch4_window12_384\n",
        "\n",
        "    # What names?\n",
        "    nn_model_name = f\"model_tgtdiv100_f{args.fold}.bin\"\n",
        "    \n",
        "    # Get neural net model file\n",
        "    if args.source == 's3':\n",
        "        nn_prefix_name = 'train_effb0_with_MLP_23Nov/'\n",
        "        try:\n",
        "            s3r.Object(BUCKET_NAME, nn_prefix_name+nn_model_name).download_file(nn_model_name)\n",
        "            # Load it to reap the hard work we put in \n",
        "            model.load(nn_model_name, device=\"cuda\", weights_only=True)\n",
        "        except:\n",
        "            print(\"Did not manage to download model bin for fold. Exiting\")\n",
        "            break\n",
        "\n",
        "    if args.source == 'kaggle':\n",
        "        # Load it to reap the hard work we put in \n",
        "        model.load(args.model_path+nn_model_name, device=\"cuda\", weights_only=True)\n",
        "\n",
        "    # Give \n",
        "    svr_name = f\"SVR_fold_{fold}.pkl\"\n",
        "    \n",
        "    # Setting up dataframe for this particular fold\n",
        "    df_train = df[df.kfold != args.fold].reset_index(drop=True)\n",
        "    df_valid = df[df.kfold == args.fold].reset_index(drop=True)\n",
        "\n",
        "    # Adding in full path so model will take this in to know where to expect to find images for training\n",
        "    train_img_paths = [args.image_path+f\"{x}.jpg\" for x in df_train[\"Id\"].values]\n",
        "    valid_img_paths = [args.image_path+f\"{x}.jpg\" for x in df_valid[\"Id\"].values]\n",
        "\n",
        "    # Train SVR if no path defined\n",
        "    if args.isSVRfitting:\n",
        "\n",
        "        # Extracting embeddings from trained model\n",
        "        print('Extracting train embedding...')\n",
        "        \n",
        "        LOAD_SVR_FROM_PATH = None\n",
        "\n",
        "        # Why are we using test_aug (same as valid_aug)\n",
        "        # Targets divided by 100 here due to use of sigmoid at final output. 100 is multiplied back after.\n",
        "        train_dataset = PawpularDataset(\n",
        "            image_paths=train_img_paths,\n",
        "            dense_features=df_train[dense_features].values,\n",
        "            targets=df_train.Pawpularity.values/100.00,\n",
        "            augmentations=valid_aug,\n",
        "        )\n",
        "\n",
        "        # Record our predictions from nerual net \n",
        "        train_predictions = model.predict(train_dataset, batch_size=2*args.batch_size, n_jobs=-1)\n",
        "\n",
        "        # Prepare a container to store embeddings\n",
        "        embed = np.array([]).reshape((0,128+12))\n",
        "\n",
        "        # For each prediction, we store all rows and cols 1 to the rest.\n",
        "        # This step takes ~7 mins. Pred does not seem to occur until actually accessing the preds.\n",
        "        for preds in train_predictions:\n",
        "            embed = np.concatenate([embed,preds[:,1:]],axis=0)\n",
        "        \n",
        "        # Fit RAPIDS SVR\n",
        "        print('Fitting SVR...')\n",
        "        clf = SVR(C=20.0)\n",
        "        clf.fit(embed.astype('float32'), df_train.Pawpularity.values.astype('int32'))\n",
        "\n",
        "        # Save RAPIDS SVR\n",
        "        print('Saving SVR...')\n",
        "        pickle.dump(clf, open(svr_name, \"wb\"))\n",
        "        \n",
        "        if args.sink == 's3':\n",
        "            s3r.meta.client.upload_file(f\"SVR_fold_\"+str(fold)+\".pkl\", BUCKET_NAME, f\"trained_SVR_23Nov/SVR_fold_\"+str(fold)+\".pkl\")\n",
        "            \n",
        "        if args.sink == 'kaggle':\n",
        "            print('Model output sink set to kaggle. Are you sure? *future improvement*')\n",
        "    \n",
        "    # Load SVR if we have it.\n",
        "    else:\n",
        "        \n",
        "        print('Loading SVR...',LOAD_SVR_FROM_PATH+svr_name)\n",
        "        \n",
        "        if args.source == 's3':\n",
        "            s3r.Object(BUCKET_NAME, \"trained_SVR_23Nov/SVR_fold_0.pkl\").download_file(\"SVR_fold_0.pkl\")\n",
        "            LOAD_SVR_FROM_PATH = './'\n",
        "            \n",
        "        if args.source == 'kaggle':\n",
        "            if args.isTEST:\n",
        "                LOAD_SVR_FROM_PATH = args.model_path\n",
        "        \n",
        "        clf = pickle.load(open(LOAD_SVR_FROM_PATH+svr_name, \"rb\"))\n",
        "   \n",
        "    if args.isOOF:\n",
        "        #################################################\n",
        "        # Out of Fold [OOF] Predictions, What is OOF https://is.gd/99qW1p\n",
        "        print('Predicting Out of Fold...')\n",
        "\n",
        "        super_final_oof_predictions = []\n",
        "        super_final_oof_predictions2 = []\n",
        "        super_final_oof_true = []\n",
        "\n",
        "        # Instantiate validation dataset objects\n",
        "        valid_dataset = PawpularDataset(\n",
        "            image_paths=valid_img_paths,\n",
        "            dense_features=df_valid[dense_features].values,\n",
        "            targets=df_valid['Pawpularity'].values/100.00,\n",
        "            augmentations=valid_aug,\n",
        "        )\n",
        "\n",
        "        valid_predictions = model.predict(valid_dataset, batch_size=2*args.batch_size, n_jobs=-1)\n",
        "\n",
        "        final_oof_predictions = []\n",
        "        embed = np.array([]).reshape((0,128+12))\n",
        "        for preds in valid_predictions:\n",
        "            final_oof_predictions.extend(preds[:,:1].ravel().tolist())\n",
        "            embed = np.concatenate([embed,preds[:,1:]],axis=0)\n",
        "\n",
        "        final_oof_predictions = [x * 100 for x in final_oof_predictions] # [sigmoid(x) * 100 for x in final_oof_predictions]\n",
        "        final_oof_predictions2 = clf.predict(embed)    \n",
        "        super_final_oof_predictions.append(final_oof_predictions)\n",
        "        super_final_oof_predictions2.append(final_oof_predictions2)\n",
        "\n",
        "        final_oof_true = df_valid['Pawpularity'].values\n",
        "        super_final_oof_true.append(final_oof_true)\n",
        "\n",
        "        #################################################\n",
        "        # Compute RMSE\n",
        "        rsme = np.sqrt( np.mean( (super_final_oof_true[-1] - np.array(super_final_oof_predictions[-1]))**2.0 ) )\n",
        "        print('\\nNN RSME =',rsme,'\\n')\n",
        "\n",
        "        rsme = np.sqrt( np.mean( (super_final_oof_true[-1] - np.array(super_final_oof_predictions2[-1]))**2.0 ) )\n",
        "        print('SVR RSME =',rsme,'\\n')\n",
        "\n",
        "        w = 0.5\n",
        "        oof2 = (1-w)*np.array(super_final_oof_predictions[-1]) + w*np.array(super_final_oof_predictions2[-1])\n",
        "        rsme = np.sqrt( np.mean( (super_final_oof_true[-1] - oof2)**2.0 ) )\n",
        "        print('Ensemble RSME =',rsme,'\\n')\n",
        "    \n",
        "    if args.isTEST:\n",
        "        #################################################\n",
        "        # Testing our predictions\n",
        "        print('Predicting test...')\n",
        "\n",
        "        super_final_predictions = []\n",
        "        super_final_predictions2 = []\n",
        "\n",
        "        # Initialize test dataset. Actual test images are only used after code submit\n",
        "        # Notice also that we init targets as array of ones\n",
        "        test_dataset = PawpularDataset(\n",
        "            image_paths=test_img_paths,\n",
        "            dense_features=df_test[dense_features].values,\n",
        "            targets=np.ones(len(test_img_paths)),\n",
        "            augmentations=test_aug,\n",
        "        )\n",
        "        \n",
        "        # Store our predictions of test images\n",
        "        test_predictions = model.predict(test_dataset, batch_size=2*args.batch_size, n_jobs=-1)\n",
        "\n",
        "        # Store emebddings from test predictions.\n",
        "        # What is final_test_predictions? \n",
        "        final_test_predictions = []\n",
        "        embed = np.array([]).reshape((0,128+12))\n",
        "        for preds in test_predictions: #tqdm\n",
        "            final_test_predictions.extend(preds[:,:1].ravel().tolist())\n",
        "            embed = np.concatenate([embed,preds[:,1:]],axis=0)\n",
        "\n",
        "\n",
        "        # Final compute for predictions out of NN\n",
        "        final_test_predictions = [x * 100 for x in final_test_predictions]\n",
        "\n",
        "        # Take embeddings from NN, and use SVR to get predictions.\n",
        "        final_test_predictions2 = clf.predict(embed)\n",
        "\n",
        "        # Store both predictions above\n",
        "        super_final_predictions.append(final_test_predictions)\n",
        "        super_final_predictions2.append(final_test_predictions2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:20.857062Z",
          "iopub.execute_input": "2021-11-23T15:19:20.857397Z",
          "iopub.status.idle": "2021-11-23T15:19:20.866600Z",
          "shell.execute_reply.started": "2021-11-23T15:19:20.857356Z",
          "shell.execute_reply": "2021-11-23T15:19:20.865816Z"
        },
        "trusted": true,
        "id": "et_hQ6kFdCLG"
      },
      "source": [
        "if args.isOOF:\n",
        "\n",
        "    true = np.hstack(super_final_oof_true)\n",
        "\n",
        "    oof = np.hstack(super_final_oof_predictions)\n",
        "    rsme = np.sqrt( np.mean( (oof - true)**2.0 ))\n",
        "    print('Overall CV NN head RSME =',rsme)\n",
        "\n",
        "    oof2 = np.hstack(super_final_oof_predictions2)\n",
        "    rsme = np.sqrt( np.mean( (oof2 - true)**2.0 ))\n",
        "    print('Overall CV SVR head RSME =',rsme)\n",
        "\n",
        "    oof3 = (1-w)*oof + w*oof2\n",
        "    rsme = np.sqrt( np.mean( (oof3 - true)**2.0 ))\n",
        "    print('Overall CV Ensemble heads RSME with 50% NN and 50% SVR =',rsme)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:20.868072Z",
          "iopub.execute_input": "2021-11-23T15:19:20.868388Z",
          "iopub.status.idle": "2021-11-23T15:19:20.897896Z",
          "shell.execute_reply.started": "2021-11-23T15:19:20.868336Z",
          "shell.execute_reply": "2021-11-23T15:19:20.897225Z"
        },
        "trusted": true,
        "id": "AmU3hCa4dCLH"
      },
      "source": [
        "# FORCE SVR WEIGHT TO LOWER VALUE TO HELP PUBLIC LB\n",
        "best_w = 0.5\n",
        "\n",
        "super_final_predictions = np.mean(np.column_stack(super_final_predictions), axis=1)\n",
        "super_final_predictions2 = np.mean(np.column_stack(super_final_predictions2), axis=1)\n",
        "df_test[\"Pawpularity\"] = (1-best_w)*super_final_predictions + best_w*super_final_predictions2\n",
        "df_test = df_test[[\"Id\", \"Pawpularity\"]]\n",
        "df_test.to_csv(\"submission.csv\", index=False)\n",
        "df_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
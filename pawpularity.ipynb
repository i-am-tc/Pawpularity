{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "pawpularity.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAC8KWATdCK8"
      },
      "source": [
        "Credits\n",
        "\n",
        "Tez Training by Abhishek - https://www.kaggle.com/abhishek/tez-pawpular-training/notebook\n",
        "\n",
        "RAPIDS SVR by Chris - https://www.kaggle.com/cdeotte/rapids-svr-boost-17-8?scriptVersionId=76428219\n",
        "\n",
        "Fold creation - https://www.kaggle.com/abhishek/same-old-creating-folds \n",
        "\n",
        "Install RAPIDS in Colab - https://is.gd/KOS1nC \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XezE-cWddCK_"
      },
      "source": [
        "# Arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:04.115960Z",
          "iopub.execute_input": "2021-11-23T15:19:04.116277Z",
          "iopub.status.idle": "2021-11-23T15:19:04.123119Z",
          "shell.execute_reply.started": "2021-11-23T15:19:04.116237Z",
          "shell.execute_reply": "2021-11-23T15:19:04.121953Z"
        },
        "trusted": true,
        "id": "ru3xjMRXdCLA"
      },
      "source": [
        "# Simple class to hold globals \n",
        "class args:\n",
        "    \n",
        "    # Where are you developing? So we can use P100 from both. =)\n",
        "    platform = 'colab' # colab or kaggle\n",
        "    \n",
        "    # Source is where we download dataset\n",
        "    # Sink location is where we upload and download outputs, should preserve outputs even if runtime disconns.\n",
        "    source = 'drive' # 's3' or 'drive' or 'kaggle'\n",
        "    sink = 's3' # 's3' or kaggle\n",
        "    \n",
        "    # Depending on where, we init diff paths\n",
        "    if platform == 'kaggle':\n",
        "        csv_path = \"../input/petfinder-pawpularity-score/\"\n",
        "        model_path = \"../input/pawpularitymodelfiles/\"\n",
        "        image_path = \"../input/petfinder-pawpularity-score/train/\"\n",
        "\n",
        "    if platform == 'colab':\n",
        "        csv_path = '/content/'\n",
        "        model_path = '/content/'\n",
        "        image_path = '/content/img/'\n",
        "    \n",
        "    # Model training\n",
        "    model_list = ['swin_large_patch4_window12_384'\n",
        "                  , 'swin_large_patch4_window12_384_in22k' \n",
        "                  , 'tf_efficientnet_b0_ns' \n",
        "                  , 'tf_efficientnetv2_l_in21k']\n",
        "    nn_model_name = model_list[1]\n",
        "    nn_model_filename = \"nn_\"+nn_model_name\n",
        "    svr_filename = \"SVR_\"+nn_model_name\n",
        "    nn_prefix_name = 'train_25Nov/'\n",
        "    svr_prefix_name = 'train_25Nov/'\n",
        "\n",
        "    # NN trg\n",
        "    if nn_model_name == model_list[0] or model_list[1]:\n",
        "        batch_size = 10 # Effnet: 32 ; SWIN: 10\n",
        "        image_size = 384 # EffNet: 256 ; SWIN: 384\n",
        "    else:\n",
        "        batch_size = 32 # Effnet: 32 ; SWIN: 10\n",
        "        image_size = 384 # EffNet: 256 ; SWIN: 384        \n",
        "    epochs = 20\n",
        "    fold = [0,1,2,3]\n",
        "    num_splits = 4\n",
        "    rid_dup = 1\n",
        "    low_vif = 1\n",
        "\n",
        "    # SVR trg\n",
        "    regularization = 20\n",
        "    kernel = 'rbf' # poly , linear , sigmoid\n",
        "    degree = 2\n",
        "    \n",
        "    # Weightage between NN and SVR regression results\n",
        "    nn_svr_weight = 0.5\n",
        "\n",
        "    # Activity control switches\n",
        "    isNNtraining = 1\n",
        "    extract_embeds = 1\n",
        "    isSVRfitting = 0\n",
        "    isOOF = 0\n",
        "    isSUBMIT = 0\n",
        "    \n",
        "\n",
        "    # Hardware ; Not in use 25 Nov 2021\n",
        "    isGPU = 1\n",
        "    isTPU = 0"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHHSzIvXdMx0"
      },
      "source": [
        "# Installs & Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:04.125003Z",
          "iopub.execute_input": "2021-11-23T15:19:04.125381Z",
          "iopub.status.idle": "2021-11-23T15:19:04.143710Z",
          "shell.execute_reply.started": "2021-11-23T15:19:04.125338Z",
          "shell.execute_reply": "2021-11-23T15:19:04.142983Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kqgn2kZsdCLB",
        "outputId": "0749b486-06cd-4c20-de25-a678616597fb"
      },
      "source": [
        "# Credit https://www.kaggle.com/abhishek/tez-pawpular-swin-ference\n",
        "# based on the post here: https://www.kaggle.com/c/petfinder-pawpularity-score/discussion/275094\n",
        "\n",
        "# Silencing this cell's output.\n",
        "# %%capture\n",
        "\n",
        "# Standard ones\n",
        "import os \n",
        "import sys\n",
        "import math\n",
        "import re\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "import pickle \n",
        "import subprocess\n",
        "\n",
        "# Installs & path includes. Different platforms have different defaults. Argh.\n",
        "if args.platform == 'kaggle':\n",
        "    sys.path.append(\"../input/tez-lib/\")\n",
        "    sys.path.append(\"../input/timmmaster/\")\n",
        "    \n",
        "if args.platform == 'colab':\n",
        "    subprocess.run('pip install tez', shell=True)\n",
        "    subprocess.run('pip install timm', shell=True)\n",
        "    subprocess.run('pip install boto3', shell=True)\n",
        "    subprocess.run('apt install unzip', shell=True)\n",
        "\n",
        "# Analysis\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn import datasets\n",
        "from sklearn import model_selection\n",
        "import albumentations # https://is.gd/ngksFx ; for image augmentation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm # https://is.gd/suAm9l ; Pytorch vision library\n",
        "import tez # https://git.io/J1KCq ; convenience library for using Pytorch\n",
        "from tez.callbacks import EarlyStopping\n",
        "\n",
        "# For GPU enabled support vector machines\n",
        "if args.platform == 'kaggle':\n",
        "    # We can direclty import in kaggle as it is included by default.\n",
        "    import cudf, cuml # Doc - https://is.gd/oshcbU ; \n",
        "    from cuml.svm import SVR\n",
        "    print('RAPIDS version',cuml.__version__,'\\n')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ABhJSI4e-W3"
      },
      "source": [
        "# Helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39ao7Qrb8dru"
      },
      "source": [
        "# Saving strings of name of metadata given in train.csv\n",
        "# We use this when we instantiate a Pawpular dataset, to tell it what metadata to expect.\n",
        "# From data exlore: some metadata features have very high VIF (measure of multicolinearity)\n",
        "dense_features_full = [\n",
        "    'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n",
        "    'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'\n",
        "]\n",
        "\n",
        "# Metadata features with VIF < 2\n",
        "dense_features_lowVIF = [\n",
        "    'Subject Focus', 'Action', 'Accessory',\n",
        "    'Group', 'Collage', 'Info', 'Blur'\n",
        "]\n",
        "\n",
        "if args.low_vif:\n",
        "    dense_features = dense_features_lowVIF\n",
        "else:\n",
        "    dense_features = dense_features_full\n",
        "\n",
        "# Duplicate image handling\n",
        "duplicates = ['13d215b4c71c3dc603cd13fc3ec80181', '373c763f5218610e9b3f82b12ada8ae5', '5ef7ba98fc97917aec56ded5d5c2b099', '67e97de8ec7ddcda59a58b027263cdcc', '839087a28fa67bf97cdcaf4c8db458ef', 'a8f044478dba8040cc410e3ec7514da1', '1feb99c2a4cac3f3c4f8a4510421d6f5', '264845a4236bc9b95123dde3fb809a88', '3c50a7050df30197e47865d08762f041', 'def7b2f2685468751f711cc63611e65b', '37ae1a5164cd9ab4007427b08ea2c5a3', '3f0222f5310e4184a60a7030da8dc84b', '5a642ecc14e9c57a05b8e010414011f2', 'c504568822c53675a4f425c8e5800a36', '2a8409a5f82061e823d06e913dee591c', '86a71a412f662212fe8dcd40fdaee8e6', '3c602cbcb19db7a0998e1411082c487d', 'a8bb509cd1bd09b27ff5343e3f36bf9e', '0422cd506773b78a6f19416c98952407', '0b04f9560a1f429b7c48e049bcaffcca', '68e55574e523cf1cdc17b60ce6cc2f60', '9b3267c1652691240d78b7b3d072baf3', '1059231cf2948216fcc2ac6afb4f8db8', 'bca6811ee0a78bdcc41b659624608125', '5da97b511389a1b62ef7a55b0a19a532', '8ffde3ae7ab3726cff7ca28697687a42', '78a02b3cb6ed38b2772215c0c0a7f78e', 'c25384f6d93ca6b802925da84dfa453e', '08440f8c2c040cf2941687de6dc5462f', 'bf8501acaeeedc2a421bac3d9af58bb7', '0c4d454d8f09c90c655bd0e2af6eb2e5', 'fe47539e989df047507eaa60a16bc3fd', '5a5c229e1340c0da7798b26edf86d180', 'dd042410dc7f02e648162d7764b50900', '871bb3cbdf48bd3bfd5a6779e752613e', '988b31dd48a1bc867dbc9e14d21b05f6', 'dbf25ce0b2a5d3cb43af95b2bd855718', 'e359704524fa26d6a3dcd8bfeeaedd2e', '43bd09ca68b3bcdc2b0c549fd309d1ba', '6ae42b731c00756ddd291fa615c822a1', '43ab682adde9c14adb7c05435e5f2e0e', '9a0238499efb15551f06ad583a6fa951', 'a9513f7f0c93e179b87c01be847b3e4c', 'b86589c3e85f784a5278e377b726a4d4', '38426ba3cbf5484555f2b5e9504a6b03', '6cb18e0936faa730077732a25c3dfb94', '589286d5bfdc1b26ad0bf7d4b7f74816', 'cd909abf8f425d7e646eebe4d3bf4769', '9f5a457ce7e22eecd0992f4ea17b6107', 'b967656eb7e648a524ca4ffbbc172c06', 'b148cbea87c3dcc65a05b15f78910715', 'e09a818b7534422fb4c688f12566e38f', '3877f2981e502fe1812af38d4f511fd2', '902786862cbae94e890a090e5700298b', '8f20c67f8b1230d1488138e2adbb0e64', 'b190f25b33bd52a8aae8fd81bd069888', '221b2b852e65fe407ad5fd2c8e9965ef', '94c823294d542af6e660423f0348bf31', '2b737750362ef6b31068c4a4194909ed', '41c85c2c974cc15ca77f5ababb652f84', '01430d6ae02e79774b651175edd40842', '6dc1ae625a3bfb50571efedc0afc297c', '72b33c9c368d86648b756143ab19baeb', '763d66b9cf01069602a968e573feb334', '03d82e64d1b4d99f457259f03ebe604d', 'dbc47155644aeb3edd1bd39dba9b6953', '851c7427071afd2eaf38af0def360987', 'b49ad3aac4296376d7520445a27726de', '54563ff51aa70ea8c6a9325c15f55399', 'b956edfd0677dd6d95de6cb29a85db9c', '87c6a8f85af93b84594a36f8ffd5d6b8', 'd050e78384bd8b20e7291b3efedf6a5b', '04201c5191c3b980ae307b20113c8853', '16d8e12207ede187e65ab45d7def117b']\n",
        "\n",
        "# Helper: sigmoid, math for 1 of many types of activation function.\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + math.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def gauss(x):\n",
        "    return np.exp(-(x*x))\n",
        "\n",
        "# Clear out memory that GPU is hogging\n",
        "def clear_trash():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    subprocess.run('export PYTORCH_CUDA_ALLOC_CONF=0', shell=True)\n",
        "    return\n",
        "\n",
        "def rid_duplicates(df):\n",
        "    for id in duplicates:\n",
        "        df = df.drop(df[df.Id == id].index)\n",
        "    return df.reset_index(drop=True)\n",
        "\n",
        "# Credit: https://is.gd/zwVtpa ; Takes in pandas dataframe, returns pd Series ready for cross-val\n",
        "def create_folds(data, num_splits):\n",
        "\n",
        "    # Added col \"kfold\", assigned val -1 for starters\n",
        "    data[\"kfold\"] = -1 \n",
        "\n",
        "    # https://is.gd/wgZBrH & https://www.statisticshowto.com/?p=7678\n",
        "    # Rule-thumb for setting bin sizes, most likely Sturge rule used here\n",
        "    # bins are akin to giving labels to equal width range of pscore. \n",
        "    num_bins = int(np.floor(1 + np.log2(len(data)))) \n",
        "    \n",
        "    # https://is.gd/U6dVgC, sort by pscore, segment into num_bins, output into col \"bins\": bin 1, bin 2 ...\n",
        "    data.loc[:, \"bins\"] = pd.cut(data[\"Pawpularity\"], bins=num_bins, labels=False) \n",
        "\n",
        "    # Doc: https://is.gd/6MJHst, What: https://is.gd/pG4oqH , Why: https://is.gd/bVYvsS\n",
        "    # Instantiate a stratifed k-fold object, 10 folds gives us 90%/10% split betw train/valid\n",
        "    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
        "    \n",
        "    # .split generates indices for split betw train/valid, accord to bin labels\n",
        "    # enumerate(kf.split(X=data, y=data.bins.values)) is of shape (10,2) , col 1 is fold label, col 2 is a tuple of entry-index of (train, valid)\n",
        "    # What's enumerate: https://www.programiz.com/node/600 : adds col 1's fold label\n",
        "    # For each fold label, use .loc to assign list of valid-index corresponding fold label\n",
        "    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n",
        "        data.loc[v_, 'kfold'] = f\n",
        "    \n",
        "    # Delte bin label col as we no longer need it.\n",
        "    data = data.drop(\"bins\", axis=1)\n",
        "\n",
        "    # Return an edited dataframe\n",
        "    return data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AsTNOOK8Zxw"
      },
      "source": [
        "# Data & Model Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:04.373265Z",
          "iopub.execute_input": "2021-11-23T15:19:04.373491Z",
          "iopub.status.idle": "2021-11-23T15:19:04.395839Z",
          "shell.execute_reply.started": "2021-11-23T15:19:04.373465Z",
          "shell.execute_reply": "2021-11-23T15:19:04.395126Z"
        },
        "trusted": true,
        "id": "krLJwTwWdCLD"
      },
      "source": [
        "# Class to represent dataset's image , metadata & labels , sushi rolled into 1 object. \n",
        "# Returns dict of torch tensors form of image, metadata & labels\n",
        "class PawpularDataset:\n",
        "\n",
        "    # Method called when object is instantiated, here, we set what params to take in\n",
        "    # https://www.geeksforgeeks.org/?p=360686 \n",
        "    def __init__(self, image_paths, dense_features, targets, augmentations):\n",
        "        self.image_paths = image_paths\n",
        "        self.dense_features = dense_features\n",
        "        self.targets = targets\n",
        "        self.augmentations = augmentations\n",
        "    \n",
        "    # Gives length of an attribute here when using len() on an instance\n",
        "    # https://www.analyticsvidhya.com/?p=83204#h2_5 \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    \n",
        "    # https://www.geeksforgeeks.org/?p=385574 \n",
        "    # https://www.codespeedy.com/?p=28884 : setitem vs getitem \n",
        "    # __getitem__ called when we -> InstanceOfDataSet[0] \n",
        "    # __setitem__ called when we -> InstanceOfDataSet[0] = *something*\n",
        "    def __getitem__(self, item):\n",
        "\n",
        "        # Use cv2 to read image with path\n",
        "        image = cv2.imread(self.image_paths[item])\n",
        "\n",
        "        # Why: https://is.gd/eSX1xj\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
        "        \n",
        "        # If augm obj passed in, we perform augm and return augmented images\n",
        "        if self.augmentations is not None:\n",
        "\n",
        "            # https://is.gd/01LBW6, do the augm\n",
        "            augmented = self.augmentations(image=image)\n",
        "\n",
        "            # Albumentation returns a dictionary after augm, only a single key. \n",
        "            image = augmented[\"image\"]\n",
        "\n",
        "        # https://is.gd/nPDsJf, why are we transposing here? \n",
        "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
        "\n",
        "        # Get image's dense_features\n",
        "        features = self.dense_features[item, :]\n",
        "\n",
        "        # Get image's targets\n",
        "        targets = self.targets[item]\n",
        "        \n",
        "        return {\n",
        "            \"image\": torch.tensor(image, dtype=torch.float),\n",
        "            \"features\": torch.tensor(features, dtype=torch.float),\n",
        "            \"targets\": torch.tensor(targets, dtype=torch.float),\n",
        "        }\n",
        "    \n",
        "class PawpularModel(tez.Model):\n",
        "    def __init__(self, get_pretrained, model_string):\n",
        "        \n",
        "        # Inherit tez.Model, https://git.io/J1VAJ , tez.Model in turn inherits nn.Module\n",
        "        super().__init__()\n",
        "        \n",
        "        self.get_pretrained = get_pretrained\n",
        "        self.model_string = model_string\n",
        "        \n",
        "        # Use of pretrained tf_efficientnet_b0_ns as base\n",
        "        # https://is.gd/pOZdAH : List of models supported by timm \n",
        "        # https://git.io/J1VNa : Useful links to Efficient net family\n",
        "        # https://is.gd/zCLbMt: What create_model does\n",
        "        # https://git.io/J1weq : where create_model is in timm's code\n",
        "        # create_model > load_checkpoint > load_pretrained \n",
        "        # A sort of object returned, seems to download from somewhere, rather than actually creating it. Bunch of checks along the way        \n",
        "        self.model = timm.create_model(self.model_string, pretrained=self.get_pretrained, in_chans=3)\n",
        "        \n",
        "        # self.model.classifier is a module of the model above, a pre-trained model created with timm, consisting of deep sequence of diff modules\n",
        "        # https://git.io/J1wmw : EfficientNet class in Pytorch ; https://git.io/J1wmH : nn.Module class in torch\n",
        "        # Change output feature value from default of 1280 to 128\n",
        "        if re.match('tf_efficient', self.model_string): # CNN\n",
        "            self.model.classifier = nn.Linear(self.model.classifier.in_features, 128)\n",
        "        if re.match('swin_', self.model_string): # Transformers\n",
        "            self.model.head = nn.Linear(self.model.head.in_features, 128) \n",
        "        if re.match('vit_', self.model_string): # Transformers\n",
        "            self.model.head = nn.Linear(self.model.head.in_features, 128)\n",
        "\n",
        "        # https://is.gd/joBsSF : form of regularization technique, zero-ing elements in a tensor with a Bernoulli distri with param p=0.1\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "        # https://is.gd/TnIKzT : Defining MLP layers\n",
        "        # 128+12 input as from model above\n",
        "        # 1 output since we're doing regreession of pscore here. \n",
        "        self.dense1 = nn.Linear(128+len(dense_features), 64)\n",
        "        self.dense2 = nn.Linear(64, 1)\n",
        "        \n",
        "        self.step_scheduler_after = \"epoch\"\n",
        "\n",
        "    # RMSE - https://is.gd/1700Cd \n",
        "    # RMSE reporting when target is passed in. \n",
        "    def monitor_metrics(self, outputs, targets):\n",
        "        if args.isNNtraining:\n",
        "            outputs = outputs.cpu().detach().numpy()\n",
        "            targets = targets.cpu().detach().numpy()\n",
        "            rmse = metrics.mean_squared_error(targets, outputs, squared=False)\n",
        "            return {\"rmse\": rmse}\n",
        "        else:\n",
        "            return \n",
        "\n",
        "    # https://is.gd/gMaUfO : control Learning rate decay, similar to that we learnt in SGD-Module 2-MLPy in MM, more spohisticated. \n",
        "    def fetch_scheduler(self):\n",
        "        sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            self.optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1\n",
        "        )\n",
        "        return sch\n",
        "\n",
        "    # https://is.gd/N2C9eB : ADAM optimizer\n",
        "    def fetch_optimizer(self):\n",
        "        opt = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
        "        return opt\n",
        "\n",
        "    def forward(self, image, features, targets=None):\n",
        "        \n",
        "        # send an image into a model\n",
        "        x1 = self.model(image)\n",
        "        \n",
        "        # Apply dropout @ 10%\n",
        "        x = self.dropout(x1)\n",
        "        \n",
        "        # Concatenate dropout output & metadata features given, \n",
        "        x = torch.cat([x, features], dim=1)\n",
        "        \n",
        "        # A MLP's single layer: 128+1 in , 64 out\n",
        "        x = self.dense1(x)\n",
        "        \n",
        "        # A MLP's single layer: 64 in, 1 out\n",
        "        x = self.dense2(x)\n",
        "\n",
        "        if not args.isNNtraining:\n",
        "            # https://is.gd/YIDi42\n",
        "            # If we're not training, return MLP output, image, metadata features\n",
        "            x = torch.cat([x, x1, features], dim=1)\n",
        "\n",
        "        if targets is not None:\n",
        "            loss = nn.MSELoss()(x, targets.view(-1, 1))\n",
        "            metrics = self.monitor_metrics(x, targets)\n",
        "            return x, loss, metrics        \n",
        "        \n",
        "        return x, 0, {}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meHK5hTgzPAH"
      },
      "source": [
        "# Data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwHts-Qq6DZ1"
      },
      "source": [
        "## Setup sources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:04.145839Z",
          "iopub.execute_input": "2021-11-23T15:19:04.146127Z",
          "iopub.status.idle": "2021-11-23T15:19:04.155646Z",
          "shell.execute_reply.started": "2021-11-23T15:19:04.146092Z",
          "shell.execute_reply": "2021-11-23T15:19:04.154900Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee-hxp7KdCLC",
        "outputId": "81285832-707d-44ef-88f2-605afb048f90"
      },
      "source": [
        "if args.source == 'drive':\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True) \n",
        "\n",
        "if args.source == 's3' or args.sink == 's3':\n",
        "\n",
        "    # Setting up for AWS S3\n",
        "\n",
        "    import boto3 # Doc - https://is.gd/K9zpHw \n",
        "    from getpass import getpass\n",
        "\n",
        "    BUCKET_NAME = 'pawpularity-data'\n",
        "\n",
        "    # Using getpass from here https://is.gd/yN9yap for security. \n",
        "\n",
        "    print('Input AWS access key ID:')\n",
        "    aws_access_key_id = getpass()\n",
        "    print('Input AWS secret access key:')\n",
        "    aws_secret_access_key = getpass()\n",
        "\n",
        "    s3r = boto3.resource('s3', \n",
        "                        aws_access_key_id = aws_access_key_id, \n",
        "                        aws_secret_access_key= aws_secret_access_key) \n",
        "\n",
        "    s3c = boto3.client('s3', \n",
        "                        aws_access_key_id = aws_access_key_id, \n",
        "                        aws_secret_access_key= aws_secret_access_key)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Input AWS access key ID:\n",
            "··········\n",
            "Input AWS secret access key:\n",
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkClMo0h6L0R"
      },
      "source": [
        "## CSV, dataframe & images from Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:04.171219Z",
          "iopub.execute_input": "2021-11-23T15:19:04.171547Z",
          "iopub.status.idle": "2021-11-23T15:19:04.215929Z",
          "shell.execute_reply.started": "2021-11-23T15:19:04.171511Z",
          "shell.execute_reply": "2021-11-23T15:19:04.215238Z"
        },
        "trusted": true,
        "id": "QYV-P5iJdCLD"
      },
      "source": [
        "# Get CSVs and images.\n",
        "\n",
        "if args.source == 's3':\n",
        "    \n",
        "    # Download CSVs from S3\n",
        "    s3r.Object(BUCKET_NAME, 'train.csv').download_file('train.csv')\n",
        "    s3r.Object(BUCKET_NAME, 'test.csv').download_file('test.csv')\n",
        "    s3r.Object(BUCKET_NAME, 'train_10folds.csv').download_file('train_10folds.csv')\n",
        "\n",
        "    # Read original csv\n",
        "    df = pd.read_csv(args.path+\"train_10folds.csv\")\n",
        "\n",
        "    # Read test csv\n",
        "    df_test = pd.read_csv(args.path+\"test.csv\")\n",
        "\n",
        "if args.source == 'drive':\n",
        "    \n",
        "    # Unzip and copy over\n",
        "    subprocess.run('unzip -j /content/drive/MyDrive/Pawpularity_TC_Drive/petfinder-pawpularity-score.zip -d img/', shell=True)\n",
        "    subprocess.run('mv /content/img/train.csv /content', shell=True)\n",
        "    subprocess.run('mv /content/img/test.csv /content', shell=True)\n",
        "    subprocess.run('rm -rf /content/img/test /content/img/sample_submission.csv', shell=True)\n",
        "    \n",
        "    # Read original csv\n",
        "    df = create_folds( pd.read_csv(\"train.csv\") , num_splits=10)\n",
        "\n",
        "    # Read test csv\n",
        "    df_test = pd.read_csv(\"test.csv\")\n",
        "    \n",
        "if args.source == 'kaggle':\n",
        "    # Read original csv\n",
        "    df = create_folds( pd.read_csv(args.csv_path+\"train.csv\") , num_splits=10)\n",
        "\n",
        "    # Read test csv\n",
        "    df_test = pd.read_csv(args.csv_path+\"test.csv\")\n",
        "\n",
        "if args.rid_dup:\n",
        "    df = create_folds(rid_duplicates(pd.read_csv(args.csv_path+\"train.csv\")) , num_splits=args.num_splits)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh6z7OpL6QxL"
      },
      "source": [
        "## Images from S3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:04.397527Z",
          "iopub.execute_input": "2021-11-23T15:19:04.399141Z",
          "iopub.status.idle": "2021-11-23T15:19:04.410956Z",
          "shell.execute_reply.started": "2021-11-23T15:19:04.399109Z",
          "shell.execute_reply": "2021-11-23T15:19:04.410153Z"
        },
        "trusted": true,
        "id": "_n8GKuKddCLE"
      },
      "source": [
        "# Credit and edits made: https://is.gd/jEPCoQ\n",
        "# mknod : https://is.gd/OVXJsr \n",
        "# https://masnun.com/?p=3009: Tutorial on Python's concurrent & futures\n",
        "# https://is.gd/S1x8tA : When to ThresdPool and ProcessPool\n",
        "\n",
        "# Get images from S3. Peeled out because this takes longest and costs tiny $$\n",
        "if args.source == 's3':\n",
        "    from concurrent import futures\n",
        "\n",
        "    prefix = 'img'\n",
        "    bucket_name = 'pawpularity-data'\n",
        "    max_workers = 20000\n",
        "\n",
        "    # Saving strings of keys of images ; Since we want to be S3 compatible, we'll need prefix + \"/\" + image name + .jpg\n",
        "    img_keys = [prefix+\"/\"+str(x)+\".jpg\" for x in df[\"Id\"].values]\n",
        "    abs_path = os.path.abspath('')\n",
        "\n",
        "    try:\n",
        "        os.makedirs('./'+ prefix)\n",
        "    except:\n",
        "        print(\"Directory already created. Moving on ...\")\n",
        "\n",
        "    def fetch(key):\n",
        "        file = f'{abs_path}/{key}'\n",
        "        os.mknod(file, mode=384)  \n",
        "        with open(file, 'wb') as data:\n",
        "            s3c.download_fileobj(bucket_name, key, data)\n",
        "        return file\n",
        "\n",
        "    def fetch_all(keys):\n",
        "\n",
        "        with futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "\n",
        "            print(\"Hang on ... submitting file downloads\")\n",
        "\n",
        "            future_to_key = {executor.submit(fetch, key): key for key in keys}\n",
        "\n",
        "            print(\"All URLs submitted.\")\n",
        "\n",
        "            for future in futures.as_completed(future_to_key):\n",
        "\n",
        "                key = future_to_key[future]\n",
        "                exception = future.exception()\n",
        "\n",
        "                if not exception:\n",
        "                    yield key, future.result()\n",
        "                else:\n",
        "                    yield key, exception\n",
        "\n",
        "    i=0\n",
        "    for key, result in fetch_all(img_keys):\n",
        "        i+=1\n",
        "\n",
        "    print('Number of images downloaded: ', i)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zxy6Fr4VfhJi"
      },
      "source": [
        "# Image Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:04.412148Z",
          "iopub.execute_input": "2021-11-23T15:19:04.412741Z",
          "iopub.status.idle": "2021-11-23T15:19:04.424240Z",
          "shell.execute_reply.started": "2021-11-23T15:19:04.412700Z",
          "shell.execute_reply": "2021-11-23T15:19:04.423553Z"
        },
        "trusted": true,
        "id": "YzHgko9kdCLF"
      },
      "source": [
        "# Albumentation is a lib for image augmentation operations. \n",
        "# .Compose is the way we define an augmentation pipe line with Albumentation, See https://is.gd/tn84mO , https://is.gd/07Sh95 \n",
        "# list of transforms supported : https://is.gd/weLPx8\n",
        "\n",
        "train_aug = albumentations.Compose(\n",
        "    [\n",
        "        albumentations.Resize(args.image_size, args.image_size, p=1), # https://is.gd/J26M4R\n",
        "        albumentations.HueSaturationValue( # https://is.gd/NlVygf\n",
        "            hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5\n",
        "        ),\n",
        "        albumentations.RandomBrightnessContrast( # https://is.gd/rSiA5P\n",
        "            brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5\n",
        "        ),\n",
        "        albumentations.Normalize( # https://is.gd/GQ4pFo, values used here are defaults\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225],\n",
        "            max_pixel_value=255.0,\n",
        "            p=1.0,\n",
        "        ),\n",
        "    ],\n",
        "    p=1.0,\n",
        ")\n",
        "\n",
        "# Validation set performs only resizing & normalize. Presumably to match images from train set\n",
        "valid_aug = albumentations.Compose(\n",
        "    [\n",
        "        albumentations.Resize(args.image_size, args.image_size, p=1),\n",
        "        albumentations.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225],\n",
        "            max_pixel_value=255.0,\n",
        "            p=1.0,\n",
        "        ),\n",
        "    ],\n",
        "    p=1.0,\n",
        ")\n",
        "\n",
        "test_aug = albumentations.Compose(\n",
        "    [\n",
        "        albumentations.Resize(args.image_size, args.image_size, p=1),\n",
        "        albumentations.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225],\n",
        "            max_pixel_value=255.0,\n",
        "            p=1.0,\n",
        "        ),\n",
        "    ],\n",
        "    p=1.0,\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jGC8rJCiRJb"
      },
      "source": [
        "# timm.list_models()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvEIomW0feC8"
      },
      "source": [
        "# Neural Net Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:04.427069Z",
          "iopub.execute_input": "2021-11-23T15:19:04.427546Z",
          "iopub.status.idle": "2021-11-23T15:19:05.380699Z",
          "shell.execute_reply.started": "2021-11-23T15:19:04.427510Z",
          "shell.execute_reply": "2021-11-23T15:19:05.378695Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFN1VBkbdCLF",
        "outputId": "72fa7af7-c7f1-4888-fe50-bcce817e0f2a"
      },
      "source": [
        "clear_trash()\n",
        "\n",
        "if args.isNNtraining:\n",
        "\n",
        "    # Instantiate model, in prep for train.\n",
        "    model = PawpularModel(True, args.nn_model_name)\n",
        "    \n",
        "    for i in args.fold:\n",
        "        print(\"\\nTraining fold number:\", i)\n",
        "\n",
        "        # What name?\n",
        "        nn_model_filename = args.nn_model_filename+f\"_f{i}.bin\"\n",
        "        nn_prefix_name = args.nn_prefix_name\n",
        "\n",
        "        ##############################################################################\n",
        "        # Setting up dataframe for this particular fold\n",
        "        df_train = df[df.kfold != i].reset_index(drop=True)\n",
        "        df_valid = df[df.kfold == i].reset_index(drop=True)\n",
        "\n",
        "        # Adding in full path so model will take this in to know where to expect to find images for training\n",
        "        # remove '/content/' if running on Kaggle \n",
        "        train_img_paths = [args.image_path+f\"{x}.jpg\" for x in df_train[\"Id\"].values]\n",
        "        valid_img_paths = [args.image_path+f\"{x}.jpg\" for x in df_valid[\"Id\"].values]\n",
        "\n",
        "        # Instantiating PawpularDataset objects: 1 for training, 1 for validation\n",
        "        train_dataset = PawpularDataset(\n",
        "            image_paths=train_img_paths,\n",
        "            dense_features=df_train[dense_features].values,\n",
        "            targets=df_train.Pawpularity.values/100.0,\n",
        "            augmentations=train_aug,\n",
        "        )\n",
        "\n",
        "        valid_dataset = PawpularDataset(\n",
        "            image_paths=valid_img_paths,\n",
        "            dense_features=df_valid[dense_features].values,\n",
        "            targets=df_valid.Pawpularity.values/100.0,\n",
        "            augmentations=valid_aug,\n",
        "        )\n",
        "\n",
        "        # Defining an early stop callback function\n",
        "        es = EarlyStopping(\n",
        "            monitor=\"valid_rmse\",\n",
        "            model_path= nn_model_filename,\n",
        "            patience=3,\n",
        "            mode=\"min\",\n",
        "            save_weights_only=True,\n",
        "        )\n",
        "\n",
        "        # Hit the gym and train!!\n",
        "        model.fit(\n",
        "            train_dataset,\n",
        "            valid_dataset=valid_dataset,\n",
        "            train_bs=args.batch_size,\n",
        "            valid_bs=2*args.batch_size,\n",
        "            device=\"cuda\",\n",
        "            epochs=args.epochs,\n",
        "            callbacks=[es],\n",
        "            fp16=True,\n",
        "        )\n",
        "        \n",
        "        if args.sink == 's3':\n",
        "            # Send model bin file to S3\n",
        "            s3r.meta.client.upload_file(nn_model_filename, BUCKET_NAME, nn_prefix_name+nn_model_filename)\n",
        "            print(\"\\nUploaded trained model to S3. Fold : \", i)\n",
        "        \n",
        "        if args.sink == 'kaggle':\n",
        "            print('Model output sink set to kaggle. Are you sure? Rmb to manual download it *future improvement')\n",
        "\n",
        "clear_trash()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 738/738 [17:33<00:00,  1.43s/it, loss=0.0396, rmse=0.19, stage=train]\n",
            "  0%|          | 0/123 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 123/123 [02:04<00:00,  1.02s/it, loss=0.0353, rmse=0.183, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score improved (inf --> 0.183226142108925). Saving model!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/738 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 738/738 [17:32<00:00,  1.43s/it, loss=0.0326, rmse=0.173, stage=train]\n",
            "  0%|          | 0/123 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 123/123 [02:04<00:00,  1.01s/it, loss=0.035, rmse=0.182, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score improved (0.183226142108925 --> 0.1819872346108522). Saving model!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/738 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 738/738 [17:32<00:00,  1.43s/it, loss=0.0225, rmse=0.144, stage=train]\n",
            "  0%|          | 0/123 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 123/123 [02:04<00:00,  1.01s/it, loss=0.0398, rmse=0.195, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EarlyStopping counter: 1 out of 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/738 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 738/738 [17:32<00:00,  1.43s/it, loss=0.0124, rmse=0.107, stage=train]\n",
            "  0%|          | 0/123 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 123/123 [02:04<00:00,  1.01s/it, loss=0.0372, rmse=0.188, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EarlyStopping counter: 2 out of 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/738 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 738/738 [17:32<00:00,  1.43s/it, loss=0.0075, rmse=0.0834, stage=train]\n",
            "  0%|          | 0/123 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 123/123 [02:04<00:00,  1.01s/it, loss=0.0364, rmse=0.187, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EarlyStopping counter: 3 out of 3\n",
            "\n",
            "Uploaded trained model to S3. Fold :  [0, 1, 2, 3]\n",
            "\n",
            "Training fold number: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/738 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 738/738 [17:31<00:00,  1.43s/it, loss=0.0044, rmse=0.0641, stage=train]\n",
            "  0%|          | 0/123 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 123/123 [02:03<00:00,  1.01s/it, loss=0.0352, rmse=0.183, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score improved (inf --> 0.18277397782095078). Saving model!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/738 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 738/738 [17:32<00:00,  1.43s/it, loss=0.00255, rmse=0.0486, stage=train]\n",
            "  0%|          | 0/123 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 123/123 [02:04<00:00,  1.01s/it, loss=0.0352, rmse=0.183, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EarlyStopping counter: 1 out of 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/738 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 738/738 [17:32<00:00,  1.43s/it, loss=0.00157, rmse=0.0381, stage=train]\n",
            "  0%|          | 0/123 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 123/123 [02:04<00:00,  1.01s/it, loss=0.0347, rmse=0.182, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score improved (0.18277397782095078 --> 0.181542109062032). Saving model!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/738 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 738/738 [17:32<00:00,  1.43s/it, loss=0.0011, rmse=0.0318, stage=train]\n",
            "  0%|          | 0/123 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 123/123 [02:04<00:00,  1.01s/it, loss=0.0349, rmse=0.182, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EarlyStopping counter: 1 out of 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/738 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 738/738 [17:32<00:00,  1.43s/it, loss=0.000809, rmse=0.0272, stage=train]\n",
            "  0%|          | 0/123 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 123/123 [02:04<00:00,  1.01s/it, loss=0.0345, rmse=0.181, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EarlyStopping counter: 2 out of 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/738 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 738/738 [17:32<00:00,  1.43s/it, loss=0.0197, rmse=0.13, stage=train]\n",
            "  0%|          | 0/123 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 123/123 [02:03<00:00,  1.01s/it, loss=0.0407, rmse=0.197, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EarlyStopping counter: 3 out of 3\n",
            "\n",
            "Uploaded trained model to S3. Fold :  [0, 1, 2, 3]\n",
            "\n",
            "Training fold number: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/738 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 738/738 [17:32<00:00,  1.43s/it, loss=0.0105, rmse=0.098, stage=train]\n",
            "  0%|          | 0/123 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 123/123 [02:04<00:00,  1.01s/it, loss=0.0346, rmse=0.181, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score improved (inf --> 0.1813010650315905). Saving model!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/738 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 738/738 [17:32<00:00,  1.43s/it, loss=0.00544, rmse=0.071, stage=train]\n",
            "  0%|          | 0/123 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 123/123 [02:04<00:00,  1.01s/it, loss=0.0362, rmse=0.185, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EarlyStopping counter: 1 out of 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/738 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 738/738 [17:32<00:00,  1.43s/it, loss=0.00365, rmse=0.058, stage=train]\n",
            "  0%|          | 0/123 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 123/123 [02:04<00:00,  1.01s/it, loss=0.0347, rmse=0.182, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EarlyStopping counter: 2 out of 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/738 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 738/738 [17:32<00:00,  1.43s/it, loss=0.00258, rmse=0.0488, stage=train]\n",
            "  0%|          | 0/123 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 123/123 [02:04<00:00,  1.01s/it, loss=0.0346, rmse=0.181, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EarlyStopping counter: 3 out of 3\n",
            "\n",
            "Uploaded trained model to S3. Fold :  [0, 1, 2, 3]\n",
            "\n",
            "Training fold number: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/738 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            " 19%|█▊        | 137/738 [03:16<14:16,  1.42s/it, loss=0.00217, rmse=0.0441, stage=train]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHhK4WyzdCLG"
      },
      "source": [
        "# Neural Net + Support Vector Regression "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:05.383580Z",
          "iopub.execute_input": "2021-11-23T15:19:05.384239Z",
          "iopub.status.idle": "2021-11-23T15:19:20.855211Z",
          "shell.execute_reply.started": "2021-11-23T15:19:05.384158Z",
          "shell.execute_reply": "2021-11-23T15:19:20.854112Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "XPsd_5_adCLG",
        "outputId": "730781e2-2529-478c-c322-874ca07d2cfd"
      },
      "source": [
        "# Incorporating RAPIDS's Support Vector Regression\n",
        "clear_trash()\n",
        "\n",
        "# Placeholders for Out Of Folda & Test predictions; \n",
        "# Needs to be here as we take mean of all folds and use as valid or submit results.\n",
        "allfolds_oof_nnreg_preds = []\n",
        "allfolds_oof_svrreg_preds = []\n",
        "allfolds_oof_validation_tgts = []\n",
        "allfolds_submit_nnreg_preds = []\n",
        "allfolds_submit_svrreg_preds = []\n",
        "\n",
        "# Adding in full path so model will take this in to know where to expect to find images for training\n",
        "test_img_paths = [args.csv_path+\"test/\"+f\"{x}.jpg\" for x in df_test[\"Id\"].values]\n",
        "\n",
        "# for fold in range(10):\n",
        "for i in args.fold:\n",
        "\n",
        "    # We re-initialize the model at every fold\n",
        "    model = PawpularModel(False, args.nn_model_name) # swin_large_patch4_window12_384 , tf_efficientnetv2_b0 , tf_efficientnetv2_l_in21ft1k\n",
        "\n",
        "    # What names?\n",
        "    nn_model_filename = args.nn_model_filename+f\"_f{i}.bin\"\n",
        "    svr_filename = args.svr_filename+f\"_f{i}.pkl\"\n",
        "    nn_prefix_name = args.nn_prefix_name\n",
        "    svr_prefix_name = args.svr_prefix_name\n",
        "\n",
        "##############################################################################    \n",
        "    # Are we doing a submission? Are we doing an SVR fitting?\n",
        "    if not args.isSVRfitting and not args.isSUBMIT:\n",
        "        if args.sink == 's3':\n",
        "            try:\n",
        "                s3r.Object(BUCKET_NAME, svr_prefix_name+svr_filename).download_file(svr_filename)\n",
        "                LOAD_SVR_FROM_PATH = './'\n",
        "            except:\n",
        "                print('File name : ', nn_prefix_name+nn_model_filename)\n",
        "                print(\"Did not manage to download model bin for fold. Exiting\")                \n",
        "        if args.sink == 'kaggle':\n",
        "            LOAD_SVR_FROM_PATH = args.model_path\n",
        "        \n",
        "    if args.isSUBMIT:\n",
        "        LOAD_SVR_FROM_PATH = args.model_path\n",
        "    \n",
        "    # Get neural net model file to reap the hard work we put in \n",
        "    if args.sink == 's3':\n",
        "        try:\n",
        "            # Downloading it from S3\n",
        "            s3r.Object(BUCKET_NAME, nn_prefix_name+nn_model_filename).download_file(nn_model_filename) \n",
        "            model.load(nn_model_filename, device=\"cuda\", weights_only=True)\n",
        "        except:\n",
        "            print('File name : ', nn_prefix_name+nn_model_filename)\n",
        "            print(\"Did not manage to download model bin for fold. Exiting\")\n",
        "            break\n",
        "\n",
        "    if args.sink == 'kaggle':\n",
        "        try:\n",
        "            # For this to work, must manually \"Add Data\"\n",
        "            model.load(args.model_path+nn_model_filename, device=\"cuda\", weights_only=True)\n",
        "        except:\n",
        "            print(\"Did not manage to load model bin for fold. Sure its there? Exiting\")\n",
        "            break            \n",
        "    \n",
        "    # Setting up dataframe for this particular fold\n",
        "    df_train = df[df.kfold != i].reset_index(drop=True)\n",
        "    df_valid = df[df.kfold == i].reset_index(drop=True)\n",
        "\n",
        "    # Adding in full path so model will take this in to know where to expect to find images for training\n",
        "    train_img_paths = [args.image_path+f\"{x}.jpg\" for x in df_train[\"Id\"].values]\n",
        "    valid_img_paths = [args.image_path+f\"{x}.jpg\" for x in df_valid[\"Id\"].values]\n",
        "\n",
        "\n",
        "##############################################################################    \n",
        "    if args.extract_embeds:\n",
        "        # Extracting embeddings from trained model\n",
        "        print('Extracting train embedding...')\n",
        "        \n",
        "        # Why are we using test_aug (same as valid_aug)\n",
        "        # Targets divided by 100 here due to use of sigmoid at final output. 100 is multiplied back after.\n",
        "        train_dataset = PawpularDataset(\n",
        "            image_paths=train_img_paths,\n",
        "            dense_features=df_train[dense_features].values,\n",
        "            targets=df_train.Pawpularity.values/100.00,\n",
        "            augmentations=valid_aug,\n",
        "        )\n",
        "\n",
        "        # Record our predictions from nerual net \n",
        "        train_predictions = model.predict(train_dataset, batch_size=2*args.batch_size, n_jobs=-1)\n",
        "\n",
        "        # Prepare a container to store embeddings\n",
        "        embed = np.array([]).reshape((0,128+len(dense_features)))\n",
        "\n",
        "        # For each prediction, we store all rows and cols 1 to the rest.\n",
        "        # This step takes ~7 mins. Pred does not seem to occur until actually accessing the preds.\n",
        "        for preds in train_predictions:\n",
        "            embed = np.concatenate([embed,preds[:,1:]],axis=0)\n",
        "            \n",
        "        pickle.dump(embed, open('embed_'+nn_model_filename,\"wb\"))\n",
        "\n",
        "        clear_trash()\n",
        "\n",
        "##############################################################################        \n",
        "    # Train SVR if no path defined\n",
        "    if args.isSVRfitting:\n",
        "        \n",
        "        # Fit RAPIDS SVR\n",
        "        print('Fitting SVR...')\n",
        "\n",
        "        # Init SVR machine. C value here is the regularization parameter ; https://is.gd/hsYtDM \n",
        "        # Poly with deg2 performed better than RBF. Cache size above 10k unstable. Gamma: scale is better\n",
        "        clf = SVR(C=args.regularization, kernel=args.kernel, degree=args.degree, cache_size=8192)        \n",
        "\n",
        "        # Open extracted embedding pickel\n",
        "        embed_pkl = pickle.load(open('embed_'+nn_model_filename, \"rb\"))\n",
        "\n",
        "        # Fit SVR machine. Essentially Multi In, Single Out here.\n",
        "        clf.fit(embed_pkl.astype('float32'), df_train.Pawpularity.values.astype('int32'))\n",
        "\n",
        "        # Save RAPIDS SVR\n",
        "        print('Saving SVR...')\n",
        "        pickle.dump(clf, open(svr_filename, \"wb\"))\n",
        "        \n",
        "\n",
        "        if args.sink == 's3':\n",
        "            s3r.meta.client.upload_file(svr_filename, BUCKET_NAME, svr_prefix_name+svr_filename)\n",
        "            \n",
        "        if args.sink == 'kaggle':\n",
        "            print('Model output sink set to kaggle. Are you sure? *future improvement*')\n",
        "    \n",
        "    # Load SVR if we have it.\n",
        "    else:\n",
        "        \n",
        "        print('Loading SVR...',LOAD_SVR_FROM_PATH+svr_filename)\n",
        "        \n",
        "        if args.source == 's3':\n",
        "            s3r.Object(BUCKET_NAME, svr_prefix_name+svr_filename).download_file(svr_filename)\n",
        "            LOAD_SVR_FROM_PATH = './'\n",
        "            \n",
        "        if args.source == 'kaggle':\n",
        "            if args.isSUBMIT:\n",
        "                LOAD_SVR_FROM_PATH = args.model_path\n",
        "        \n",
        "        clf = pickle.load(open(LOAD_SVR_FROM_PATH+svr_filename, \"rb\"))\n",
        "\n",
        "        clear_trash()\n",
        "\n",
        "##############################################################################\n",
        "    if args.isOOF:\n",
        "        # Out of Fold [OOF] Predictions, What is OOF https://is.gd/99qW1p\n",
        "        print('Predicting Out of Fold...')\n",
        "\n",
        "        # Instantiate validation dataset objects\n",
        "        valid_dataset = PawpularDataset(\n",
        "            image_paths=valid_img_paths,\n",
        "            dense_features=df_valid[dense_features].values,\n",
        "            targets=df_valid['Pawpularity'].values/100.00,\n",
        "            augmentations=valid_aug,\n",
        "        )\n",
        "\n",
        "        valid_predictions = model.predict(valid_dataset, batch_size=2*args.batch_size, n_jobs=-1)\n",
        "\n",
        "        sglfold_oof_nnreg_pred = []\n",
        "        embed = np.array([]).reshape((0,128+len(dense_features)))\n",
        "        for preds in valid_predictions:\n",
        "            sglfold_oof_nnreg_pred.extend(preds[:,:1].ravel().tolist())\n",
        "            embed = np.concatenate([embed,preds[:,1:]],axis=0)\n",
        "\n",
        "        sglfold_oof_nnreg_pred = [x * 100 for x in sglfold_oof_nnreg_pred] # [sigmoid(x) * 100 for x in sglfold_oof_nnreg_pred]\n",
        "        sglfold_oof_svrreg_pred = clf.predict(embed)    \n",
        "        allfolds_oof_nnreg_preds.append(sglfold_oof_nnreg_pred)\n",
        "        allfolds_oof_svrreg_preds.append(sglfold_oof_svrreg_pred)\n",
        "\n",
        "        sglfold_oof_validation_tgts = df_valid['Pawpularity'].values\n",
        "        allfolds_oof_validation_tgts.append(sglfold_oof_validation_tgts)\n",
        "\n",
        "        #################################################\n",
        "        # Compute RMSE\n",
        "        rsme = np.sqrt( np.mean( (allfolds_oof_validation_tgts[-1] - np.array(allfolds_oof_nnreg_preds[-1]))**2.0 ) )\n",
        "        print('\\nNN RSME =',rsme,'\\n')\n",
        "\n",
        "        rsme = np.sqrt( np.mean( (allfolds_oof_validation_tgts[-1] - np.array(allfolds_oof_svrreg_preds[-1]))**2.0 ) )\n",
        "        print('SVR RSME =',rsme,'\\n')\n",
        "\n",
        "        oof2 = (1-args.nn_svr_weight)*np.array(allfolds_oof_nnreg_preds[-1]) + args.nn_svr_weight*np.array(allfolds_oof_svrreg_preds[-1])\n",
        "        rsme = np.sqrt( np.mean( (allfolds_oof_validation_tgts[-1] - oof2)**2.0 ) )\n",
        "        print('Ensemble RSME =',rsme,'\\n')\n",
        "\n",
        "        clear_trash()\n",
        "\n",
        "##############################################################################\n",
        "    if args.isSUBMIT:\n",
        "        # Testing our predictions\n",
        "        print('Predicting test...')\n",
        "\n",
        "        # Initialize test dataset. Actual test images are only used after code submit\n",
        "        # Notice also that we init targets as array of ones\n",
        "        test_dataset = PawpularDataset(\n",
        "            image_paths=test_img_paths,\n",
        "            dense_features=df_test[dense_features].values,\n",
        "            targets=np.ones(len(test_img_paths)),\n",
        "            augmentations=test_aug,\n",
        "        )\n",
        "        \n",
        "        # Store our predictions of test images\n",
        "        test_predictions = model.predict(test_dataset, batch_size=2*args.batch_size, n_jobs=-1)\n",
        "\n",
        "        # Store emebddings from test predictions.\n",
        "        sglfold_submit_nnreg_pred = []\n",
        "        embed = np.array([]).reshape((0,128+len(dense_features)))\n",
        "        for preds in test_predictions: #tqdm\n",
        "            sglfold_submit_nnreg_pred.extend(preds[:,:1].ravel().tolist())\n",
        "            embed = np.concatenate([embed,preds[:,1:]],axis=0)\n",
        "\n",
        "\n",
        "        # Final compute for predictions out of NN\n",
        "        sglfold_submit_nnreg_pred = [x * 100 for x in sglfold_submit_nnreg_pred]\n",
        "\n",
        "        # Take embeddings from NN, and use SVR to get predictions.\n",
        "        sglfold_submit_svrreg_pred = clf.predict(embed)\n",
        "\n",
        "        # Store both predictions above\n",
        "        allfolds_submit_nnreg_preds.append(sglfold_submit_nnreg_pred)\n",
        "        allfolds_submit_svrreg_preds.append(sglfold_submit_svrreg_pred)\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ClientError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b4784e360f60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misSVRfitting\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misSUBMIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msink\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m's3'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0ms3r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUCKET_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvr_prefix_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msvr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mLOAD_SVR_FROM_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msink\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'kaggle'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/boto3/s3/inject.py\u001b[0m in \u001b[0;36mobject_download_file\u001b[0;34m(self, Filename, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    313\u001b[0m     return self.meta.client.download_file(\n\u001b[1;32m    314\u001b[0m         \u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         ExtraArgs=ExtraArgs, Callback=Callback, Config=Config)\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/boto3/s3/inject.py\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(self, Bucket, Key, Filename, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    171\u001b[0m         return transfer.download_file(\n\u001b[1;32m    172\u001b[0m             \u001b[0mbucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             extra_args=ExtraArgs, callback=Callback)\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/boto3/s3/transfer.py\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(self, bucket, key, filename, extra_args, callback)\u001b[0m\n\u001b[1;32m    313\u001b[0m             bucket, key, filename, extra_args, subscribers)\n\u001b[1;32m    314\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0;31m# This is for backwards compatibility where when retries are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;31m# exceeded we need to throw the same error from boto3 instead of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;31m# however if a KeyboardInterrupt is raised we want want to exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# out of this and propogate the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;31m# final result.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/s3transfer/tasks.py\u001b[0m in \u001b[0;36m_main\u001b[0;34m(self, transfer_future, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;31m# Call the submit method to start submitting tasks to execute the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;31m# transfer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_submit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;31m# If there was an exception raised during the submission of task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/s3transfer/download.py\u001b[0m in \u001b[0;36m_submit\u001b[0;34m(self, client, config, osutil, request_executor, io_executor, transfer_future, bandwidth_limiter)\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                 \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m             )\n\u001b[1;32m    345\u001b[0m             transfer_future.meta.provide_transfer_size(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    390\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mClientError\u001b[0m: An error occurred (404) when calling the HeadObject operation: Not Found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:20.857062Z",
          "iopub.execute_input": "2021-11-23T15:19:20.857397Z",
          "iopub.status.idle": "2021-11-23T15:19:20.866600Z",
          "shell.execute_reply.started": "2021-11-23T15:19:20.857356Z",
          "shell.execute_reply": "2021-11-23T15:19:20.865816Z"
        },
        "trusted": true,
        "id": "et_hQ6kFdCLG"
      },
      "source": [
        "if args.isOOF:\n",
        "\n",
        "    true = np.hstack(allfolds_oof_validation_tgts)\n",
        "\n",
        "    oof = np.hstack(allfolds_oof_nnreg_preds)\n",
        "    rsme = np.sqrt( np.mean( (oof - true)**2.0 ))\n",
        "    print('Overall CV NN head RSME =',rsme)\n",
        "\n",
        "    oof2 = np.hstack(allfolds_oof_svrreg_preds)\n",
        "    rsme = np.sqrt( np.mean( (oof2 - true)**2.0 ))\n",
        "    print('Overall CV SVR head RSME =',rsme)\n",
        "\n",
        "    oof3 = (1-args.nn_svr_weight)*oof + args.nn_svr_weight*oof2\n",
        "    rsme = np.sqrt( np.mean( (oof3 - true)**2.0 ))\n",
        "    print('Overall CV Ensemble heads RSME with 50% NN and 50% SVR =',rsme)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:20.868072Z",
          "iopub.execute_input": "2021-11-23T15:19:20.868388Z",
          "iopub.status.idle": "2021-11-23T15:19:20.897896Z",
          "shell.execute_reply.started": "2021-11-23T15:19:20.868336Z",
          "shell.execute_reply": "2021-11-23T15:19:20.897225Z"
        },
        "trusted": true,
        "id": "AmU3hCa4dCLH"
      },
      "source": [
        "if args.isSUBMIT:\n",
        "    # FORCE SVR WEIGHT TO LOWER VALUE TO HELP PUBLIC LB\n",
        "\n",
        "    allfolds_submit_nnreg_preds = np.mean(np.column_stack(allfolds_submit_nnreg_preds), axis=1)\n",
        "    allfolds_submit_svrreg_preds = np.mean(np.column_stack(allfolds_submit_svrreg_preds), axis=1)\n",
        "    df_test[\"Pawpularity\"] = (1-args.nn_svr_weight)*allfolds_submit_nnreg_preds + args.nn_svr_weight*allfolds_submit_svrreg_preds\n",
        "    df_test = df_test[[\"Id\", \"Pawpularity\"]]\n",
        "    df_test.to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "pawpularity.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAC8KWATdCK8"
      },
      "source": [
        "Credits\n",
        "\n",
        "Tez Training by Abhishek - https://www.kaggle.com/abhishek/tez-pawpular-training/notebook\n",
        "\n",
        "RAPIDS SVR by Chris - https://www.kaggle.com/cdeotte/rapids-svr-boost-17-8?scriptVersionId=76428219\n",
        "\n",
        "Fold creation - https://www.kaggle.com/abhishek/same-old-creating-folds \n",
        "\n",
        "Install RAPIDS in Colab - https://is.gd/KOS1nC \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XezE-cWddCK_"
      },
      "source": [
        "# Arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:04.115960Z",
          "iopub.execute_input": "2021-11-23T15:19:04.116277Z",
          "iopub.status.idle": "2021-11-23T15:19:04.123119Z",
          "shell.execute_reply.started": "2021-11-23T15:19:04.116237Z",
          "shell.execute_reply": "2021-11-23T15:19:04.121953Z"
        },
        "trusted": true,
        "id": "ru3xjMRXdCLA"
      },
      "source": [
        "# Simple class to hold a bunch of globals \n",
        "class args:\n",
        "    \n",
        "    # Where you developing? So we can use P100 from both. =)\n",
        "    platform = 'colab' # colab or kaggle\n",
        "    \n",
        "    # Source is where we download dataset\n",
        "    # Sink location is where we upload and download outputs, should preserve outputs even if runtime disconns.\n",
        "    source = 'drive' # 's3' or 'drive' or 'kaggle'\n",
        "    sink = 's3' # 's3' or kaggle\n",
        "    \n",
        "    # Depending on where, these paths will come in handy\n",
        "    if platform == 'kaggle':\n",
        "        csv_path = \"../input/petfinder-pawpularity-score/\"\n",
        "        model_path = \"../input/pawpularity-model-files/\"\n",
        "        image_path = \"../input/petfinder-pawpularity-score/train/\"\n",
        "\n",
        "    if platform == 'colab':\n",
        "        csv_path = '/content/'\n",
        "        model_path = '/content/'\n",
        "        image_path = '/content/img/'\n",
        "    \n",
        "    # Model training\n",
        "    batch_size = 10 # Effnet: 32 ; SWIN: 10\n",
        "    image_size = 384 # EffNet: 256 ; SWIN: 384\n",
        "    epochs = 20\n",
        "    fold = 0\n",
        "    \n",
        "    # Activity control switches\n",
        "    isNNtraining = 1\n",
        "    isSVRfitting = 0\n",
        "    isGPU = 1\n",
        "    isTPU = 0\n",
        "    isOOF = 1\n",
        "    isSUBMIT = 0"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHHSzIvXdMx0"
      },
      "source": [
        "# Installs & Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:04.125003Z",
          "iopub.execute_input": "2021-11-23T15:19:04.125381Z",
          "iopub.status.idle": "2021-11-23T15:19:04.143710Z",
          "shell.execute_reply.started": "2021-11-23T15:19:04.125338Z",
          "shell.execute_reply": "2021-11-23T15:19:04.142983Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kqgn2kZsdCLB",
        "outputId": "074d44b3-b661-4b69-def6-06697cc46297"
      },
      "source": [
        "# Credit https://www.kaggle.com/abhishek/tez-pawpular-swin-ference\n",
        "# based on the post here: https://www.kaggle.com/c/petfinder-pawpularity-score/discussion/275094\n",
        "\n",
        "# Silencing this cell's output.\n",
        "# %%capture\n",
        "\n",
        "# Standard ones\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import pickle \n",
        "import subprocess\n",
        "\n",
        "# Installs & path includes. Different platforms have different defaults. Argh.\n",
        "if args.platform == 'kaggle':\n",
        "    sys.path.append(\"../input/tez-lib/\")\n",
        "    sys.path.append(\"../input/timmmaster/\")\n",
        "    \n",
        "if args.platform == 'colab':\n",
        "    subprocess.run('pip install tez', shell=True)\n",
        "    subprocess.run('pip install timm', shell=True)\n",
        "    subprocess.run('pip install boto3', shell=True)\n",
        "    subprocess.run('apt install unzip', shell=True)\n",
        "\n",
        "# Analysis\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn import datasets\n",
        "from sklearn import model_selection\n",
        "import albumentations # https://is.gd/ngksFx ; for image augmentation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm # https://is.gd/suAm9l ; Pytorch vision library\n",
        "import tez # https://git.io/J1KCq ; convenience library for using Pytorch\n",
        "from tez.callbacks import EarlyStopping\n",
        "\n",
        "# For GPU enabled support vector machines\n",
        "if args.platform == 'kaggle':\n",
        "    # We can direclty import in kaggle as it is included by default.\n",
        "    import cudf, cuml # Doc - https://is.gd/oshcbU ; \n",
        "    from cuml.svm import SVR\n",
        "    print('RAPIDS version',cuml.__version__,'\\n')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ABhJSI4e-W3"
      },
      "source": [
        "# Helpers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39ao7Qrb8dru"
      },
      "source": [
        "# Saving strings of name of metadata given in train.csv\n",
        "# We use this when we instantiate a Pawpular dataset, to tell it what metadata to expect.\n",
        "# From data exlore: some metadata features have very high VIF (measure of multicolinearity)\n",
        "dense_features_full = [\n",
        "    'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n",
        "    'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'\n",
        "]\n",
        "\n",
        "# Metadata features with VIF < 2\n",
        "dense_features_lowVIF = [\n",
        "    'Subject Focus', 'Action', 'Accessory',\n",
        "    'Group', 'Collage', 'Info', 'Blur'\n",
        "]\n",
        "\n",
        "dense_features = dense_features_lowVIF\n",
        "\n",
        "# Duplicate image handling\n",
        "duplicates = ['13d215b4c71c3dc603cd13fc3ec80181', '373c763f5218610e9b3f82b12ada8ae5', '5ef7ba98fc97917aec56ded5d5c2b099', '67e97de8ec7ddcda59a58b027263cdcc', '839087a28fa67bf97cdcaf4c8db458ef', 'a8f044478dba8040cc410e3ec7514da1', '1feb99c2a4cac3f3c4f8a4510421d6f5', '264845a4236bc9b95123dde3fb809a88', '3c50a7050df30197e47865d08762f041', 'def7b2f2685468751f711cc63611e65b', '37ae1a5164cd9ab4007427b08ea2c5a3', '3f0222f5310e4184a60a7030da8dc84b', '5a642ecc14e9c57a05b8e010414011f2', 'c504568822c53675a4f425c8e5800a36', '2a8409a5f82061e823d06e913dee591c', '86a71a412f662212fe8dcd40fdaee8e6', '3c602cbcb19db7a0998e1411082c487d', 'a8bb509cd1bd09b27ff5343e3f36bf9e', '0422cd506773b78a6f19416c98952407', '0b04f9560a1f429b7c48e049bcaffcca', '68e55574e523cf1cdc17b60ce6cc2f60', '9b3267c1652691240d78b7b3d072baf3', '1059231cf2948216fcc2ac6afb4f8db8', 'bca6811ee0a78bdcc41b659624608125', '5da97b511389a1b62ef7a55b0a19a532', '8ffde3ae7ab3726cff7ca28697687a42', '78a02b3cb6ed38b2772215c0c0a7f78e', 'c25384f6d93ca6b802925da84dfa453e', '08440f8c2c040cf2941687de6dc5462f', 'bf8501acaeeedc2a421bac3d9af58bb7', '0c4d454d8f09c90c655bd0e2af6eb2e5', 'fe47539e989df047507eaa60a16bc3fd', '5a5c229e1340c0da7798b26edf86d180', 'dd042410dc7f02e648162d7764b50900', '871bb3cbdf48bd3bfd5a6779e752613e', '988b31dd48a1bc867dbc9e14d21b05f6', 'dbf25ce0b2a5d3cb43af95b2bd855718', 'e359704524fa26d6a3dcd8bfeeaedd2e', '43bd09ca68b3bcdc2b0c549fd309d1ba', '6ae42b731c00756ddd291fa615c822a1', '43ab682adde9c14adb7c05435e5f2e0e', '9a0238499efb15551f06ad583a6fa951', 'a9513f7f0c93e179b87c01be847b3e4c', 'b86589c3e85f784a5278e377b726a4d4', '38426ba3cbf5484555f2b5e9504a6b03', '6cb18e0936faa730077732a25c3dfb94', '589286d5bfdc1b26ad0bf7d4b7f74816', 'cd909abf8f425d7e646eebe4d3bf4769', '9f5a457ce7e22eecd0992f4ea17b6107', 'b967656eb7e648a524ca4ffbbc172c06', 'b148cbea87c3dcc65a05b15f78910715', 'e09a818b7534422fb4c688f12566e38f', '3877f2981e502fe1812af38d4f511fd2', '902786862cbae94e890a090e5700298b', '8f20c67f8b1230d1488138e2adbb0e64', 'b190f25b33bd52a8aae8fd81bd069888', '221b2b852e65fe407ad5fd2c8e9965ef', '94c823294d542af6e660423f0348bf31', '2b737750362ef6b31068c4a4194909ed', '41c85c2c974cc15ca77f5ababb652f84', '01430d6ae02e79774b651175edd40842', '6dc1ae625a3bfb50571efedc0afc297c', '72b33c9c368d86648b756143ab19baeb', '763d66b9cf01069602a968e573feb334', '03d82e64d1b4d99f457259f03ebe604d', 'dbc47155644aeb3edd1bd39dba9b6953', '851c7427071afd2eaf38af0def360987', 'b49ad3aac4296376d7520445a27726de', '54563ff51aa70ea8c6a9325c15f55399', 'b956edfd0677dd6d95de6cb29a85db9c', '87c6a8f85af93b84594a36f8ffd5d6b8', 'd050e78384bd8b20e7291b3efedf6a5b', '04201c5191c3b980ae307b20113c8853', '16d8e12207ede187e65ab45d7def117b']\n",
        "\n",
        "# Helper: sigmoid, math for 1 of many types of activation function.\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + math.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def gauss(x):\n",
        "    return np.exp(-(x*x))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AsTNOOK8Zxw"
      },
      "source": [
        "# Data & Model Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:04.373265Z",
          "iopub.execute_input": "2021-11-23T15:19:04.373491Z",
          "iopub.status.idle": "2021-11-23T15:19:04.395839Z",
          "shell.execute_reply.started": "2021-11-23T15:19:04.373465Z",
          "shell.execute_reply": "2021-11-23T15:19:04.395126Z"
        },
        "trusted": true,
        "id": "krLJwTwWdCLD"
      },
      "source": [
        "# Class to represent dataset's image , metadata & labels , sushi rolled into 1 object. \n",
        "# Returns dict of torch tensors form of image, metadata & labels\n",
        "class PawpularDataset:\n",
        "\n",
        "    # Method called when object is instantiated, here, we set what params to take in\n",
        "    # https://www.geeksforgeeks.org/?p=360686 \n",
        "    def __init__(self, image_paths, dense_features, targets, augmentations):\n",
        "        self.image_paths = image_paths\n",
        "        self.dense_features = dense_features\n",
        "        self.targets = targets\n",
        "        self.augmentations = augmentations\n",
        "    \n",
        "    # Gives length of an attribute here when using len() on an instance\n",
        "    # https://www.analyticsvidhya.com/?p=83204#h2_5 \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    \n",
        "    # https://www.geeksforgeeks.org/?p=385574 \n",
        "    # https://www.codespeedy.com/?p=28884 : setitem vs getitem \n",
        "    # __getitem__ called when we -> InstanceOfDataSet[0] \n",
        "    # __setitem__ called when we -> InstanceOfDataSet[0] = *something*\n",
        "    def __getitem__(self, item):\n",
        "\n",
        "        # Use cv2 to read image with path\n",
        "        image = cv2.imread(self.image_paths[item])\n",
        "\n",
        "        # Why: https://is.gd/eSX1xj\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
        "        \n",
        "        # If augm obj passed in, we perform augm and return augmented images\n",
        "        if self.augmentations is not None:\n",
        "\n",
        "            # https://is.gd/01LBW6, do the augm\n",
        "            augmented = self.augmentations(image=image)\n",
        "\n",
        "            # Albumentation returns a dictionary after augm, only a single key. \n",
        "            image = augmented[\"image\"]\n",
        "\n",
        "        # https://is.gd/nPDsJf, why are we transposing here? \n",
        "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
        "\n",
        "        # Get image's dense_features\n",
        "        features = self.dense_features[item, :]\n",
        "\n",
        "        # Get image's targets\n",
        "        targets = self.targets[item]\n",
        "        \n",
        "        return {\n",
        "            \"image\": torch.tensor(image, dtype=torch.float),\n",
        "            \"features\": torch.tensor(features, dtype=torch.float),\n",
        "            \"targets\": torch.tensor(targets, dtype=torch.float),\n",
        "        }\n",
        "    \n",
        "class PawpularModel(tez.Model):\n",
        "    def __init__(self, get_pretrained, model_string):\n",
        "        \n",
        "        # Inherit tez.Model, https://git.io/J1VAJ , tez.Model in turn inherits nn.Module\n",
        "        super().__init__()\n",
        "        \n",
        "        self.get_pretrained = get_pretrained\n",
        "        self.model_string = model_string\n",
        "        \n",
        "        # Use of pretrained tf_efficientnet_b0_ns as base\n",
        "        # https://is.gd/pOZdAH : List of models supported by timm \n",
        "        # https://git.io/J1VNa : Useful links to Efficient net family\n",
        "        # https://is.gd/zCLbMt: What create_model does\n",
        "        # https://git.io/J1weq : where create_model is in timm's code\n",
        "        # create_model > load_checkpoint > load_pretrained \n",
        "        # A sort of object returned, seems to download from somewhere, rather than actually creating it. Bunch of checks along the way        \n",
        "        self.model = timm.create_model(self.model_string, pretrained=self.get_pretrained, in_chans=3)\n",
        "        \n",
        "        # self.model.classifier is a module of the model above, a pre-trained model created with timm, consisting of deep sequence of diff modules\n",
        "        # https://git.io/J1wmw : EfficientNet class in Pytorch\n",
        "        # https://git.io/J1wmH : nn.Module class in torch\n",
        "        # Output features value from default of 1280 to 128\n",
        "        if re.match('tf_efficient', self.model_string):\n",
        "            self.model.classifier = nn.Linear(self.model.classifier.in_features, 128)\n",
        "        if re.match('swin_', self.model_string):\n",
        "            self.model.head = nn.Linear(self.model.head.in_features, 128) \n",
        "        if re.match('vit_', self.model_string):\n",
        "            self.model.head = nn.Linear(self.model.head.in_features, 128)\n",
        "\n",
        "        # https://is.gd/joBsSF : form of regularization technique, zero-ing elements in a tensor with a Bernoulli distri with param p=0.1\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "\n",
        "        # https://is.gd/TnIKzT : Defining MLP layers\n",
        "        # 128+12 input as from model above\n",
        "        # 1 output since we're doing regreession of pscore here. \n",
        "        self.dense1 = nn.Linear(128+len(dense_features), 64)\n",
        "        self.dense2 = nn.Linear(64, 1)\n",
        "        \n",
        "        self.step_scheduler_after = \"epoch\"\n",
        "\n",
        "    # RMSE - https://is.gd/1700Cd \n",
        "    # RMSE reporting when target is passed in. \n",
        "    def monitor_metrics(self, outputs, targets):\n",
        "        if args.isNNtraining:\n",
        "            outputs = outputs.cpu().detach().numpy()\n",
        "            targets = targets.cpu().detach().numpy()\n",
        "            rmse = metrics.mean_squared_error(targets, outputs, squared=False)\n",
        "            return {\"rmse\": rmse}\n",
        "        else:\n",
        "            return \n",
        "\n",
        "    # https://is.gd/gMaUfO : control Learning rate decay, similar to that we learnt in SGD-Module 2-MLPy in MM, more spohisticated. \n",
        "    def fetch_scheduler(self):\n",
        "        sch = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            self.optimizer, T_0=10, T_mult=1, eta_min=1e-6, last_epoch=-1\n",
        "        )\n",
        "        return sch\n",
        "\n",
        "    # https://is.gd/N2C9eB : ADAM optimizer\n",
        "    def fetch_optimizer(self):\n",
        "        opt = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
        "        return opt\n",
        "\n",
        "    def forward(self, image, features, targets=None):\n",
        "        \n",
        "        # send an image into a model\n",
        "        x1 = self.model(image)\n",
        "        \n",
        "        # Apply dropout @ 10%\n",
        "        x = self.dropout(x1)\n",
        "        \n",
        "        # Concatenate dropout output & metadata features given, \n",
        "        x = torch.cat([x, features], dim=1)\n",
        "        \n",
        "        # A MLP's single layer: 128+1 in , 64 out\n",
        "        x = self.dense1(x)\n",
        "        \n",
        "        # A MLP's single layer: 64 in, 1 out\n",
        "        x = self.dense2(x)\n",
        "\n",
        "        if not args.isNNtraining:\n",
        "            # https://is.gd/YIDi42\n",
        "            # If we're not training, return MLP output, image, metadata features\n",
        "            x = torch.cat([x, x1, features], dim=1)\n",
        "\n",
        "        if targets is not None:\n",
        "            loss = nn.MSELoss()(x, targets.view(-1, 1))\n",
        "            metrics = self.monitor_metrics(x, targets)\n",
        "            return x, loss, metrics        \n",
        "        \n",
        "        return x, 0, {}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX7kVt9i9x4q"
      },
      "source": [
        "# Data Fold Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:04.157674Z",
          "iopub.execute_input": "2021-11-23T15:19:04.157995Z",
          "iopub.status.idle": "2021-11-23T15:19:04.169704Z",
          "shell.execute_reply.started": "2021-11-23T15:19:04.157959Z",
          "shell.execute_reply": "2021-11-23T15:19:04.168981Z"
        },
        "trusted": true,
        "id": "QwooOYZOdCLC"
      },
      "source": [
        "# Credit: https://is.gd/zwVtpa \n",
        "# Helper function, takes in an original data csv, returns pd Series ready for cross-val\n",
        "\n",
        "def create_folds(data, num_splits):\n",
        "\n",
        "    # Added col \"kfold\", assigned val -1 for starters\n",
        "    data[\"kfold\"] = -1 \n",
        "\n",
        "    # https://is.gd/wgZBrH & https://www.statisticshowto.com/?p=7678\n",
        "    # Rule-thumb for setting bin sizes, most likely Sturge rule used here\n",
        "    # bins are akin to giving labels to equal width range of pscore. \n",
        "    num_bins = int(np.floor(1 + np.log2(len(data)))) \n",
        "    \n",
        "    # https://is.gd/U6dVgC, sort by pscore, segment into num_bins, output into col \"bins\": bin 1, bin 2 ...\n",
        "    data.loc[:, \"bins\"] = pd.cut(data[\"Pawpularity\"], bins=num_bins, labels=False) \n",
        "\n",
        "    # Doc: https://is.gd/6MJHst, What: https://is.gd/pG4oqH , Why: https://is.gd/bVYvsS\n",
        "    # Instantiate a stratifed k-fold cross validator object, this gives us 90%/10% split betw train/valid\n",
        "    \n",
        "    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
        "    # kf = model_selection.StratifiedShuffleSplit(n_splits=num_splits, test_size=0.10, train_size=0.90, random_state=42)\n",
        "    \n",
        "    # .split generates indices for split betw train/valid, accord to bin labels\n",
        "    # enumerate(kf.split(X=data, y=data.bins.values)) is of shape (10,2) , col 1 is fold label, col 2 is a tuple of entry-index of (train, valid)\n",
        "    # What's enumerate: https://www.programiz.com/node/600 : adds col 1's fold label\n",
        "    # For each fold label, use .loc to assign list of valid-index corresponding fold label\n",
        "    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n",
        "        data.loc[v_, 'kfold'] = f\n",
        "    \n",
        "    # Delte bin label col as we no longer need it.\n",
        "    data = data.drop(\"bins\", axis=1)\n",
        "\n",
        "    # Return an edited dataframe\n",
        "    return data"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xai21ud1Fgy"
      },
      "source": [
        "# Removing duplicate image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eabzKAYty0KG"
      },
      "source": [
        "rid_duplicates = 1\n",
        "def rid_duplicates(df):\n",
        "    for id in duplicates:\n",
        "        df = df.drop(df[df.Id == id].index)\n",
        "    return df.reset_index(drop=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meHK5hTgzPAH"
      },
      "source": [
        "# Data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwHts-Qq6DZ1"
      },
      "source": [
        "## Setup sources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:04.145839Z",
          "iopub.execute_input": "2021-11-23T15:19:04.146127Z",
          "iopub.status.idle": "2021-11-23T15:19:04.155646Z",
          "shell.execute_reply.started": "2021-11-23T15:19:04.146092Z",
          "shell.execute_reply": "2021-11-23T15:19:04.154900Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee-hxp7KdCLC",
        "outputId": "5837e5e5-696f-4045-bc16-be518b41c972"
      },
      "source": [
        "if args.source == 'drive':\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True) \n",
        "\n",
        "if args.source == 's3' or args.sink == 's3':\n",
        "\n",
        "    # Setting up for AWS S3\n",
        "\n",
        "    import boto3 # Doc - https://is.gd/K9zpHw \n",
        "    from getpass import getpass\n",
        "\n",
        "    BUCKET_NAME = 'pawpularity-data'\n",
        "\n",
        "    # Using getpass from here https://is.gd/yN9yap for security. \n",
        "\n",
        "    print('Input AWS access key ID:')\n",
        "    aws_access_key_id = getpass()\n",
        "    print('Input AWS secret access key:')\n",
        "    aws_secret_access_key = getpass()\n",
        "\n",
        "    s3r = boto3.resource('s3', \n",
        "                        aws_access_key_id = aws_access_key_id, \n",
        "                        aws_secret_access_key= aws_secret_access_key) \n",
        "\n",
        "    s3c = boto3.client('s3', \n",
        "                        aws_access_key_id = aws_access_key_id, \n",
        "                        aws_secret_access_key= aws_secret_access_key)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Input AWS access key ID:\n",
            "··········\n",
            "Input AWS secret access key:\n",
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkClMo0h6L0R"
      },
      "source": [
        "## CSV, dataframe & images from Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:04.171219Z",
          "iopub.execute_input": "2021-11-23T15:19:04.171547Z",
          "iopub.status.idle": "2021-11-23T15:19:04.215929Z",
          "shell.execute_reply.started": "2021-11-23T15:19:04.171511Z",
          "shell.execute_reply": "2021-11-23T15:19:04.215238Z"
        },
        "trusted": true,
        "id": "QYV-P5iJdCLD"
      },
      "source": [
        "# Get CSVs and images.\n",
        "\n",
        "if args.source == 's3':\n",
        "    \n",
        "    # Download CSVs from S3\n",
        "    s3r.Object(BUCKET_NAME, 'train.csv').download_file('train.csv')\n",
        "    s3r.Object(BUCKET_NAME, 'test.csv').download_file('test.csv')\n",
        "    s3r.Object(BUCKET_NAME, 'train_10folds.csv').download_file('train_10folds.csv')\n",
        "\n",
        "    # Read original csv\n",
        "    df = pd.read_csv(args.path+\"train_10folds.csv\")\n",
        "\n",
        "    # Read test csv\n",
        "    df_test = pd.read_csv(args.path+\"test.csv\")\n",
        "\n",
        "if args.source == 'drive':\n",
        "    \n",
        "    # Unzip and copy over\n",
        "    subprocess.run('unzip -j /content/drive/MyDrive/Pawpularity_TC_Drive/petfinder-pawpularity-score.zip -d img/', shell=True)\n",
        "    subprocess.run('mv /content/img/train.csv /content', shell=True)\n",
        "    subprocess.run('mv /content/img/test.csv /content', shell=True)\n",
        "    subprocess.run('rm -rf /content/img/test /content/img/sample_submission.csv', shell=True)\n",
        "    \n",
        "    # Read original csv\n",
        "    df = create_folds( pd.read_csv(\"train.csv\") , num_splits=10)\n",
        "\n",
        "    # Read test csv\n",
        "    df_test = pd.read_csv(\"test.csv\")\n",
        "    \n",
        "if args.source == 'kaggle':\n",
        "    # Read original csv\n",
        "    df = create_folds( pd.read_csv(args.csv_path+\"train.csv\") , num_splits=10)\n",
        "\n",
        "    # Read test csv\n",
        "    df_test = pd.read_csv(args.csv_path+\"test.csv\")\n",
        "\n",
        "rid_dup = 1\n",
        "if rid_dup:\n",
        "    df = create_folds(rid_duplicates(pd.read_csv(args.csv_path+\"train.csv\")) , num_splits=10)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh6z7OpL6QxL"
      },
      "source": [
        "## Images from S3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:04.397527Z",
          "iopub.execute_input": "2021-11-23T15:19:04.399141Z",
          "iopub.status.idle": "2021-11-23T15:19:04.410956Z",
          "shell.execute_reply.started": "2021-11-23T15:19:04.399109Z",
          "shell.execute_reply": "2021-11-23T15:19:04.410153Z"
        },
        "trusted": true,
        "id": "_n8GKuKddCLE"
      },
      "source": [
        "# Credit and edits made: https://is.gd/jEPCoQ\n",
        "# mknod : https://is.gd/OVXJsr \n",
        "# https://masnun.com/?p=3009: Tutorial on Python's concurrent & futures\n",
        "# https://is.gd/S1x8tA : When to ThresdPool and ProcessPool\n",
        "\n",
        "# Get images from S3. Peeled out because this takes longest and costs tiny $$\n",
        "if args.source == 's3':\n",
        "    from concurrent import futures\n",
        "\n",
        "    prefix = 'img'\n",
        "    bucket_name = 'pawpularity-data'\n",
        "    max_workers = 20000\n",
        "\n",
        "    # Saving strings of keys of images ; Since we want to be S3 compatible, we'll need prefix + \"/\" + image name + .jpg\n",
        "    img_keys = [prefix+\"/\"+str(x)+\".jpg\" for x in df[\"Id\"].values]\n",
        "    abs_path = os.path.abspath('')\n",
        "\n",
        "    try:\n",
        "        os.makedirs('./'+ prefix)\n",
        "    except:\n",
        "        print(\"Directory already created. Moving on ...\")\n",
        "\n",
        "    def fetch(key):\n",
        "        file = f'{abs_path}/{key}'\n",
        "        os.mknod(file, mode=384)  \n",
        "        with open(file, 'wb') as data:\n",
        "            s3c.download_fileobj(bucket_name, key, data)\n",
        "        return file\n",
        "\n",
        "    def fetch_all(keys):\n",
        "\n",
        "        with futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "\n",
        "            print(\"Hang on ... submitting file downloads\")\n",
        "\n",
        "            future_to_key = {executor.submit(fetch, key): key for key in keys}\n",
        "\n",
        "            print(\"All URLs submitted.\")\n",
        "\n",
        "            for future in futures.as_completed(future_to_key):\n",
        "\n",
        "                key = future_to_key[future]\n",
        "                exception = future.exception()\n",
        "\n",
        "                if not exception:\n",
        "                    yield key, future.result()\n",
        "                else:\n",
        "                    yield key, exception\n",
        "\n",
        "    i=0\n",
        "    for key, result in fetch_all(img_keys):\n",
        "        i+=1\n",
        "\n",
        "    print('Number of images downloaded: ', i)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zxy6Fr4VfhJi"
      },
      "source": [
        "# Image Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:04.412148Z",
          "iopub.execute_input": "2021-11-23T15:19:04.412741Z",
          "iopub.status.idle": "2021-11-23T15:19:04.424240Z",
          "shell.execute_reply.started": "2021-11-23T15:19:04.412700Z",
          "shell.execute_reply": "2021-11-23T15:19:04.423553Z"
        },
        "trusted": true,
        "id": "YzHgko9kdCLF"
      },
      "source": [
        "# Albumentation is a lib for image augmentation operations. \n",
        "# .Compose is the way we define an augmentation pipe line with Albumentation, See https://is.gd/tn84mO , https://is.gd/07Sh95 \n",
        "# list of transforms supported : https://is.gd/weLPx8\n",
        "\n",
        "train_aug = albumentations.Compose(\n",
        "    [\n",
        "        albumentations.Resize(args.image_size, args.image_size, p=1), # https://is.gd/J26M4R\n",
        "        albumentations.HueSaturationValue( # https://is.gd/NlVygf\n",
        "            hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5\n",
        "        ),\n",
        "        albumentations.RandomBrightnessContrast( # https://is.gd/rSiA5P\n",
        "            brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5\n",
        "        ),\n",
        "        albumentations.Normalize( # https://is.gd/GQ4pFo, values used here are defaults\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225],\n",
        "            max_pixel_value=255.0,\n",
        "            p=1.0,\n",
        "        ),\n",
        "    ],\n",
        "    p=1.0,\n",
        ")\n",
        "\n",
        "# Validation set performs only resizing & normalize. Presumably to match images from train set\n",
        "valid_aug = albumentations.Compose(\n",
        "    [\n",
        "        albumentations.Resize(args.image_size, args.image_size, p=1),\n",
        "        albumentations.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225],\n",
        "            max_pixel_value=255.0,\n",
        "            p=1.0,\n",
        "        ),\n",
        "    ],\n",
        "    p=1.0,\n",
        ")\n",
        "\n",
        "test_aug = albumentations.Compose(\n",
        "    [\n",
        "        albumentations.Resize(args.image_size, args.image_size, p=1),\n",
        "        albumentations.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225],\n",
        "            max_pixel_value=255.0,\n",
        "            p=1.0,\n",
        "        ),\n",
        "    ],\n",
        "    p=1.0,\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jGC8rJCiRJb"
      },
      "source": [
        "# timm.list_models()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvEIomW0feC8"
      },
      "source": [
        "# Neural Net Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:04.427069Z",
          "iopub.execute_input": "2021-11-23T15:19:04.427546Z",
          "iopub.status.idle": "2021-11-23T15:19:05.380699Z",
          "shell.execute_reply.started": "2021-11-23T15:19:04.427510Z",
          "shell.execute_reply": "2021-11-23T15:19:05.378695Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFN1VBkbdCLF",
        "outputId": "545f80a6-0363-475d-baf1-dab9a5f7020e"
      },
      "source": [
        "# Clear out memory that GPU is hogging\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "!export PYTORCH_CUDA_ALLOC_CONF=0\n",
        "\n",
        "# Different fold sets for convenience\n",
        "folds1 = range(10)\n",
        "folds2 = [0,1,2,3,4,5,6,7,8,9]\n",
        "folds3 = [0,1,2]\n",
        "\n",
        "if args.isNNtraining:\n",
        "\n",
        "    # Instantiate model, in prep for train.\n",
        "    model = PawpularModel(True, 'swin_large_patch4_window12_384_in22k') # or swin_large_patch4_window12_384 , tf_efficientnet_b0_ns , tf_efficientnetv2_l_in21k , \n",
        "    \n",
        "    for i in folds3:\n",
        "        args.fold = i\n",
        "        print(\"\\nTraining fold number:\", args.fold)\n",
        "\n",
        "        # What name?\n",
        "        nn_model_name = f\"nn_\"+\"swin_large_patch4_window12_384_in22k\"+f\"_f{args.fold}.bin\"\n",
        "        nn_prefix_name = 'train_24Nov/'\n",
        "\n",
        "        ##############################################################################\n",
        "        # Setting up dataframe for this particular fold\n",
        "        df_train = df[df.kfold != args.fold].reset_index(drop=True)\n",
        "        df_valid = df[df.kfold == args.fold].reset_index(drop=True)\n",
        "\n",
        "        # Adding in full path so model will take this in to know where to expect to find images for training\n",
        "        # remove '/content/' if running on Kaggle \n",
        "        train_img_paths = [args.image_path+f\"{x}.jpg\" for x in df_train[\"Id\"].values]\n",
        "        valid_img_paths = [args.image_path+f\"{x}.jpg\" for x in df_valid[\"Id\"].values]\n",
        "\n",
        "        # Instantiating PawpularDataset objects: 1 for training, 1 for validation\n",
        "        train_dataset = PawpularDataset(\n",
        "            image_paths=train_img_paths,\n",
        "            dense_features=df_train[dense_features].values,\n",
        "            targets=df_train.Pawpularity.values/100.0,\n",
        "            augmentations=train_aug,\n",
        "        )\n",
        "\n",
        "        valid_dataset = PawpularDataset(\n",
        "            image_paths=valid_img_paths,\n",
        "            dense_features=df_valid[dense_features].values,\n",
        "            targets=df_valid.Pawpularity.values/100.0,\n",
        "            augmentations=valid_aug,\n",
        "        )\n",
        "\n",
        "        # Defining an early stop callback function\n",
        "        es = EarlyStopping(\n",
        "            monitor=\"valid_rmse\",\n",
        "            model_path= nn_model_name,\n",
        "            patience=3,\n",
        "            mode=\"min\",\n",
        "            save_weights_only=True,\n",
        "        )\n",
        "\n",
        "        # Hit the gym and train!!\n",
        "        model.fit(\n",
        "            train_dataset,\n",
        "            valid_dataset=valid_dataset,\n",
        "            train_bs=args.batch_size,\n",
        "            valid_bs=2*args.batch_size,\n",
        "            device=\"cuda\",\n",
        "            epochs=args.epochs,\n",
        "            callbacks=[es],\n",
        "            fp16=True,\n",
        "        )\n",
        "        \n",
        "        if args.sink == 's3':\n",
        "            # Send model bin file to S3\n",
        "            s3r.meta.client.upload_file(nn_model_name, BUCKET_NAME, nn_prefix_name+nn_model_name)\n",
        "            print(\"\\nUploaded trained model to S3. Fold : \", args.fold)\n",
        "        \n",
        "        if args.sink == 'kaggle':\n",
        "            print('Model output sink set to kaggle. Are you sure? *future improvement')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Downloading: \"https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_large_patch4_window12_384_22k.pth\" to /root/.cache/torch/hub/checkpoints/swin_large_patch4_window12_384_22k.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training fold number: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 886/886 [20:57<00:00,  1.42s/it, loss=0.0412, rmse=0.194, stage=train]\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 50/50 [00:53<00:00,  1.07s/it, loss=0.0419, rmse=0.201, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score improved (inf --> 0.2014325937628746). Saving model!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/886 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 886/886 [20:57<00:00,  1.42s/it, loss=0.0441, rmse=0.201, stage=train]\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 50/50 [00:52<00:00,  1.04s/it, loss=0.0423, rmse=0.203, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EarlyStopping counter: 1 out of 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/886 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 886/886 [20:56<00:00,  1.42s/it, loss=0.0397, rmse=0.191, stage=train]\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 50/50 [00:52<00:00,  1.05s/it, loss=0.0367, rmse=0.188, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score improved (0.2014325937628746 --> 0.1882316116988659). Saving model!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/886 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 886/886 [20:57<00:00,  1.42s/it, loss=0.0307, rmse=0.168, stage=train]\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 50/50 [00:52<00:00,  1.05s/it, loss=0.0343, rmse=0.182, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score improved (0.1882316116988659 --> 0.1815982723236084). Saving model!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/886 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 886/886 [20:57<00:00,  1.42s/it, loss=0.0192, rmse=0.132, stage=train]\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 50/50 [00:52<00:00,  1.05s/it, loss=0.0376, rmse=0.19, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EarlyStopping counter: 1 out of 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/886 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 886/886 [20:57<00:00,  1.42s/it, loss=0.0104, rmse=0.098, stage=train]\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 50/50 [00:52<00:00,  1.05s/it, loss=0.0365, rmse=0.187, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EarlyStopping counter: 2 out of 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/886 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 886/886 [20:57<00:00,  1.42s/it, loss=0.00543, rmse=0.071, stage=train]\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 50/50 [00:52<00:00,  1.04s/it, loss=0.031, rmse=0.173, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score improved (0.1815982723236084 --> 0.17271114110946656). Saving model!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/886 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 886/886 [20:57<00:00,  1.42s/it, loss=0.00275, rmse=0.0507, stage=train]\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 50/50 [00:52<00:00,  1.05s/it, loss=0.0309, rmse=0.172, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EarlyStopping counter: 1 out of 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/886 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 886/886 [20:57<00:00,  1.42s/it, loss=0.00162, rmse=0.0386, stage=train]\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 50/50 [00:52<00:00,  1.05s/it, loss=0.0311, rmse=0.173, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EarlyStopping counter: 2 out of 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/886 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 886/886 [20:57<00:00,  1.42s/it, loss=0.00115, rmse=0.0325, stage=train]\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 50/50 [00:52<00:00,  1.05s/it, loss=0.0311, rmse=0.173, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EarlyStopping counter: 3 out of 3\n",
            "\n",
            "Uploaded trained model to S3. Fold :  0\n",
            "\n",
            "Training fold number: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/886 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 886/886 [20:57<00:00,  1.42s/it, loss=0.0172, rmse=0.124, stage=train]\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 50/50 [00:52<00:00,  1.05s/it, loss=0.0388, rmse=0.193, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score improved (inf --> 0.19303640305995942). Saving model!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/886 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 886/886 [20:57<00:00,  1.42s/it, loss=0.01, rmse=0.0963, stage=train]\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 50/50 [00:52<00:00,  1.05s/it, loss=0.0367, rmse=0.188, stage=valid]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation score improved (0.19303640305995942 --> 0.18758769512176512). Saving model!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/886 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            " 93%|█████████▎| 824/886 [19:31<01:27,  1.42s/it, loss=0.00522, rmse=0.0698, stage=train]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHhK4WyzdCLG"
      },
      "source": [
        "# Neural Net + Support Vector Regression "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:05.383580Z",
          "iopub.execute_input": "2021-11-23T15:19:05.384239Z",
          "iopub.status.idle": "2021-11-23T15:19:20.855211Z",
          "shell.execute_reply.started": "2021-11-23T15:19:05.384158Z",
          "shell.execute_reply": "2021-11-23T15:19:20.854112Z"
        },
        "trusted": true,
        "id": "XPsd_5_adCLG"
      },
      "source": [
        "# Incorporating RAPIDS's Support Vector Regression\n",
        "\n",
        "# Different fold sets for convenience\n",
        "folds1 = range(10)\n",
        "folds2 = [0,1,2,3,4,5,6,7,8,9]\n",
        "folds3 = [0]\n",
        "\n",
        "# Adding in full path so model will take this in to know where to expect to find images for training\n",
        "test_img_paths = [args.csv_path+\"test/\"+f\"{x}.jpg\" for x in df_test[\"Id\"].values]\n",
        "\n",
        "# for fold in range(10):\n",
        "for i in folds3:\n",
        "\n",
        "    # Probably not a good practice to re-init what should be a global but this makes it easier to automate file naming.\n",
        "    args.fold = i\n",
        "\n",
        "    # We re-initialize the model at every fold\n",
        "    model = PawpularModel(False, \"\") # swin_large_patch4_window12_384 , tf_efficientnetv2_b0\n",
        "\n",
        "    # What names?\n",
        "    nn_model_name = \"nn_\"+\"\"+f\"_f{args.fold}.bin\"\n",
        "    svr_name = \"SVR_\"+\"\"+f\"_f{args.fold}.pkl\"\n",
        "    nn_prefix_name = 'train_24Nov/'\n",
        "    svr_prefix_name = 'train_24Nov/'\n",
        "\n",
        "    # Are we doing a submission? Are we doing an SVR fitting?\n",
        "    if not args.isSVRfitting and not args.isSUBMIT:\n",
        "        if args.sink == 's3':\n",
        "            s3r.Object(BUCKET_NAME, svr_prefix_name+svr_name).download_file(svr_name)\n",
        "            LOAD_SVR_FROM_PATH = './'\n",
        "        if args.sink == 'kaggle':\n",
        "            LOAD_SVR_FROM_PATH = args.model_path\n",
        "        \n",
        "    if args.isSUBMIT:\n",
        "        LOAD_SVR_FROM_PATH = args.model_path\n",
        "    \n",
        "    # Get neural net model file to reap the hard work we put in \n",
        "    if args.sink == 's3':\n",
        "        try:\n",
        "            # Downloading it from S3\n",
        "            s3r.Object(BUCKET_NAME, nn_prefix_name+nn_model_name).download_file(nn_model_name) \n",
        "            model.load(nn_model_name, device=\"cuda\", weights_only=True)\n",
        "        except:\n",
        "            print(\"Did not manage to download model bin for fold. Exiting\")\n",
        "            break\n",
        "\n",
        "    if args.sink == 'kaggle':\n",
        "        try:\n",
        "            # For this to work, must manually \"Add Data\"\n",
        "            model.load(args.model_path+nn_model_name, device=\"cuda\", weights_only=True)\n",
        "        except:\n",
        "            print(\"Did not manage to load model bin for fold. Sure its there? Exiting\")\n",
        "            break            \n",
        "    \n",
        "    # Setting up dataframe for this particular fold\n",
        "    df_train = df[df.kfold != args.fold].reset_index(drop=True)\n",
        "    df_valid = df[df.kfold == args.fold].reset_index(drop=True)\n",
        "\n",
        "    # Adding in full path so model will take this in to know where to expect to find images for training\n",
        "    train_img_paths = [args.image_path+f\"{x}.jpg\" for x in df_train[\"Id\"].values]\n",
        "    valid_img_paths = [args.image_path+f\"{x}.jpg\" for x in df_valid[\"Id\"].values]\n",
        "\n",
        "\n",
        "##############################################################################    \n",
        "    #Save embedding \n",
        "    save_embeds = 1\n",
        "    if save_embeds:\n",
        "        # Extracting embeddings from trained model\n",
        "        print('Extracting train embedding...')\n",
        "        \n",
        "        LOAD_SVR_FROM_PATH = None\n",
        "\n",
        "        # Why are we using test_aug (same as valid_aug)\n",
        "        # Targets divided by 100 here due to use of sigmoid at final output. 100 is multiplied back after.\n",
        "        train_dataset = PawpularDataset(\n",
        "            image_paths=train_img_paths,\n",
        "            dense_features=df_train[dense_features].values,\n",
        "            targets=df_train.Pawpularity.values/100.00,\n",
        "            augmentations=valid_aug,\n",
        "        )\n",
        "\n",
        "        # Record our predictions from nerual net \n",
        "        train_predictions = model.predict(train_dataset, batch_size=2*args.batch_size, n_jobs=-1)\n",
        "\n",
        "        # Prepare a container to store embeddings\n",
        "        embed = np.array([]).reshape((0,128+len(dense_features)))\n",
        "\n",
        "        # For each prediction, we store all rows and cols 1 to the rest.\n",
        "        # This step takes ~7 mins. Pred does not seem to occur until actually accessing the preds.\n",
        "        for preds in train_predictions:\n",
        "            embed = np.concatenate([embed,preds[:,1:]],axis=0)\n",
        "            \n",
        "        pickle.dump(embed, open('embed_'+nn_model_name,\"wb\"))\n",
        "\n",
        "\n",
        "##############################################################################        \n",
        "    # Train SVR if no path defined\n",
        "    if args.isSVRfitting:\n",
        "        \n",
        "        # Fit RAPIDS SVR\n",
        "        print('Fitting SVR...')\n",
        "\n",
        "        # Init SVR machine. C value here is the regularization parameter ; https://is.gd/hsYtDM \n",
        "        # Poly with deg2 performed better than RBF. Cache size above 10k unstable. Gamma: scale is better\n",
        "        clf = SVR(C=10.0, kernel='poly', degree=2, cache_size=8192)        \n",
        "\n",
        "        # Open extracted embedding pickel\n",
        "        embed_pkl = pickle.load(open('embed_'+nn_model_name, \"rb\"))\n",
        "\n",
        "        # Fit SVR machine. Essentially Multi In, Single Out here.\n",
        "        clf.fit(embed_pkl.astype('float32'), df_train.Pawpularity.values.astype('int32'))\n",
        "\n",
        "        # Save RAPIDS SVR\n",
        "        print('Saving SVR...')\n",
        "        pickle.dump(clf, open(svr_name, \"wb\"))\n",
        "        \n",
        "\n",
        "        if args.sink == 's3':\n",
        "            s3r.meta.client.upload_file(svr_name, BUCKET_NAME, svr_prefix_name+svr_name)\n",
        "            \n",
        "        if args.sink == 'kaggle':\n",
        "            print('Model output sink set to kaggle. Are you sure? *future improvement*')\n",
        "    \n",
        "    # Load SVR if we have it.\n",
        "    else:\n",
        "        \n",
        "        print('Loading SVR...',LOAD_SVR_FROM_PATH+svr_name)\n",
        "        \n",
        "        if args.source == 's3':\n",
        "            s3r.Object(BUCKET_NAME, \"trained_SVR_23Nov/SVR_fold_0.pkl\").download_file(\"SVR_fold_0.pkl\")\n",
        "            LOAD_SVR_FROM_PATH = './'\n",
        "            \n",
        "        if args.source == 'kaggle':\n",
        "            if args.isSUBMIT:\n",
        "                LOAD_SVR_FROM_PATH = args.model_path\n",
        "        \n",
        "        clf = pickle.load(open(LOAD_SVR_FROM_PATH+svr_name, \"rb\"))\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "    if args.isOOF:\n",
        "        # Out of Fold [OOF] Predictions, What is OOF https://is.gd/99qW1p\n",
        "        print('Predicting Out of Fold...')\n",
        "\n",
        "        super_final_oof_predictions = []\n",
        "        super_final_oof_predictions2 = []\n",
        "        super_final_oof_true = []\n",
        "\n",
        "        # Instantiate validation dataset objects\n",
        "        valid_dataset = PawpularDataset(\n",
        "            image_paths=valid_img_paths,\n",
        "            dense_features=df_valid[dense_features].values,\n",
        "            targets=df_valid['Pawpularity'].values/100.00,\n",
        "            augmentations=valid_aug,\n",
        "        )\n",
        "\n",
        "        valid_predictions = model.predict(valid_dataset, batch_size=2*args.batch_size, n_jobs=-1)\n",
        "\n",
        "        final_oof_predictions = []\n",
        "        embed = np.array([]).reshape((0,128+len(dense_features)))\n",
        "        for preds in valid_predictions:\n",
        "            final_oof_predictions.extend(preds[:,:1].ravel().tolist())\n",
        "            embed = np.concatenate([embed,preds[:,1:]],axis=0)\n",
        "\n",
        "        final_oof_predictions = [x * 100 for x in final_oof_predictions] # [sigmoid(x) * 100 for x in final_oof_predictions]\n",
        "        final_oof_predictions2 = clf.predict(embed)    \n",
        "        super_final_oof_predictions.append(final_oof_predictions)\n",
        "        super_final_oof_predictions2.append(final_oof_predictions2)\n",
        "\n",
        "        final_oof_true = df_valid['Pawpularity'].values\n",
        "        super_final_oof_true.append(final_oof_true)\n",
        "\n",
        "        #################################################\n",
        "        # Compute RMSE\n",
        "        rsme = np.sqrt( np.mean( (super_final_oof_true[-1] - np.array(super_final_oof_predictions[-1]))**2.0 ) )\n",
        "        print('\\nNN RSME =',rsme,'\\n')\n",
        "\n",
        "        rsme = np.sqrt( np.mean( (super_final_oof_true[-1] - np.array(super_final_oof_predictions2[-1]))**2.0 ) )\n",
        "        print('SVR RSME =',rsme,'\\n')\n",
        "\n",
        "        w = 0.5\n",
        "        oof2 = (1-w)*np.array(super_final_oof_predictions[-1]) + w*np.array(super_final_oof_predictions2[-1])\n",
        "        rsme = np.sqrt( np.mean( (super_final_oof_true[-1] - oof2)**2.0 ) )\n",
        "        print('Ensemble RSME =',rsme,'\\n')\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "    if args.isSUBMIT:\n",
        "        # Testing our predictions\n",
        "        print('Predicting test...')\n",
        "\n",
        "        super_final_predictions = []\n",
        "        super_final_predictions2 = []\n",
        "\n",
        "        # Initialize test dataset. Actual test images are only used after code submit\n",
        "        # Notice also that we init targets as array of ones\n",
        "        test_dataset = PawpularDataset(\n",
        "            image_paths=test_img_paths,\n",
        "            dense_features=df_test[dense_features].values,\n",
        "            targets=np.ones(len(test_img_paths)),\n",
        "            augmentations=test_aug,\n",
        "        )\n",
        "        \n",
        "        # Store our predictions of test images\n",
        "        test_predictions = model.predict(test_dataset, batch_size=2*args.batch_size, n_jobs=-1)\n",
        "\n",
        "        # Store emebddings from test predictions.\n",
        "        # What is final_test_predictions? \n",
        "        final_test_predictions = []\n",
        "        embed = np.array([]).reshape((0,128+len(dense_features)))\n",
        "        for preds in test_predictions: #tqdm\n",
        "            final_test_predictions.extend(preds[:,:1].ravel().tolist())\n",
        "            embed = np.concatenate([embed,preds[:,1:]],axis=0)\n",
        "\n",
        "\n",
        "        # Final compute for predictions out of NN\n",
        "        final_test_predictions = [x * 100 for x in final_test_predictions]\n",
        "\n",
        "        # Take embeddings from NN, and use SVR to get predictions.\n",
        "        final_test_predictions2 = clf.predict(embed)\n",
        "\n",
        "        # Store both predictions above\n",
        "        super_final_predictions.append(final_test_predictions)\n",
        "        super_final_predictions2.append(final_test_predictions2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:20.857062Z",
          "iopub.execute_input": "2021-11-23T15:19:20.857397Z",
          "iopub.status.idle": "2021-11-23T15:19:20.866600Z",
          "shell.execute_reply.started": "2021-11-23T15:19:20.857356Z",
          "shell.execute_reply": "2021-11-23T15:19:20.865816Z"
        },
        "trusted": true,
        "id": "et_hQ6kFdCLG"
      },
      "source": [
        "if args.isOOF:\n",
        "\n",
        "    true = np.hstack(super_final_oof_true)\n",
        "\n",
        "    oof = np.hstack(super_final_oof_predictions)\n",
        "    rsme = np.sqrt( np.mean( (oof - true)**2.0 ))\n",
        "    print('Overall CV NN head RSME =',rsme)\n",
        "\n",
        "    oof2 = np.hstack(super_final_oof_predictions2)\n",
        "    rsme = np.sqrt( np.mean( (oof2 - true)**2.0 ))\n",
        "    print('Overall CV SVR head RSME =',rsme)\n",
        "\n",
        "    oof3 = (1-w)*oof + w*oof2\n",
        "    rsme = np.sqrt( np.mean( (oof3 - true)**2.0 ))\n",
        "    print('Overall CV Ensemble heads RSME with 50% NN and 50% SVR =',rsme)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-23T15:19:20.868072Z",
          "iopub.execute_input": "2021-11-23T15:19:20.868388Z",
          "iopub.status.idle": "2021-11-23T15:19:20.897896Z",
          "shell.execute_reply.started": "2021-11-23T15:19:20.868336Z",
          "shell.execute_reply": "2021-11-23T15:19:20.897225Z"
        },
        "trusted": true,
        "id": "AmU3hCa4dCLH"
      },
      "source": [
        "if args.isSUBMIT:\n",
        "    # FORCE SVR WEIGHT TO LOWER VALUE TO HELP PUBLIC LB\n",
        "    best_w = 0.5\n",
        "\n",
        "    super_final_predictions = np.mean(np.column_stack(super_final_predictions), axis=1)\n",
        "    super_final_predictions2 = np.mean(np.column_stack(super_final_predictions2), axis=1)\n",
        "    df_test[\"Pawpularity\"] = (1-best_w)*super_final_predictions + best_w*super_final_predictions2\n",
        "    df_test = df_test[[\"Id\", \"Pawpularity\"]]\n",
        "    df_test.to_csv(\"submission.csv\", index=False)\n",
        "    df_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}